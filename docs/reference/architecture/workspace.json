{
  "configuration" : { },
  "description" : "Semantic search and knowledge synthesis across 9,000 books",
  "documentation" : {
    "sections" : [ {
      "content" : "# Alexandria RAG System - Architecture Summary\r\n\r\n**One-page reference for the complete Alexandria architecture**\r\n\r\n---\r\n\r\n## System Overview\r\n\r\n**Alexandria** is a RAG (Retrieval-Augmented Generation) system that enables semantic search and knowledge synthesis across a multi-disciplinary library of ~9,000 books. It uses vector embeddings for semantic search, intelligent chunking for context preservation, and LLM integration for natural language answers.\r\n\r\n**Key Capabilities:**\r\n- üìö Browse and search Calibre library (9,000+ books)\r\n- üîç Semantic search across all ingested books\r\n- ü§ñ LLM-powered answer generation with source citations\r\n- üìä Multi-domain support (technical, psychology, philosophy, history, literature)\r\n- üéØ Universal semantic chunking (topic-aware text splitting)\r\n\r\n---\r\n\r\n## C4 Architecture Layers\r\n\r\n### Level 1: System Context\r\n\r\n```\r\n[Developer/Researcher]\r\n         ‚Üì\r\n   [Alexandria RAG System]\r\n         ‚Üì ‚Üë\r\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r\n    ‚Üì         ‚Üì              ‚Üì\r\n[Qdrant]  [OpenRouter]  [Calibre DB]\r\n```\r\n\r\n**External Systems:**\r\n- **Qdrant (192.168.0.151:6333)** - Vector database storing 384-dim embeddings\r\n- **OpenRouter API** - LLM inference (GPT-4, Claude, Llama, etc.)\r\n- **Calibre Library** - Book metadata and file storage\r\n\r\n**See:** [docs/architecture/c4/01-context.md](c4/01-context.md)\r\n\r\n---\r\n\r\n### Level 2: Containers\r\n\r\n```\r\nAlexandria RAG System\r\n‚îú‚îÄ‚îÄ Streamlit GUI (Web Browser)\r\n‚îú‚îÄ‚îÄ Scripts Package (Python Modules)\r\n‚îú‚îÄ‚îÄ File System (Book Storage + Logs)\r\n‚îî‚îÄ‚îÄ Calibre Database (SQLite)\r\n```\r\n\r\n**Container Details:**\r\n\r\n**1. Streamlit GUI** - Web interface (Python 3.14, Streamlit)\r\n   - Browse Calibre library with filters\r\n   - Trigger book ingestion with domain assignment\r\n   - Execute RAG queries with parameter controls\r\n   - Display collection statistics and manifests\r\n\r\n**2. Scripts Package** - Core business logic (Python modules)\r\n   - `ingest_books.py` - Book ingestion and chunking\r\n   - `universal_chunking.py` - Semantic text splitting\r\n   - `rag_query.py` - RAG query engine\r\n   - `collection_manifest.py` - Collection tracking\r\n   - `calibre_db.py` - Calibre integration\r\n\r\n**3. File System** - Local storage\r\n   - `ingest/` - Books waiting to be processed\r\n   - `ingested/` - Successfully processed books\r\n   - `logs/` - Collection manifests (JSON/CSV)\r\n\r\n**4. Calibre Database** - SQLite metadata store\r\n   - Book metadata (title, author, series, tags)\r\n   - File paths and formats\r\n   - Read-only access\r\n\r\n**See:** [docs/architecture/c4/02-container.md](c4/02-container.md)\r\n\r\n---\r\n\r\n### Level 3: Components (Scripts Package)\r\n\r\n```\r\nScripts Package\r\n‚îú‚îÄ‚îÄ Ingestion Engine (ingest_books.py)\r\n‚îú‚îÄ‚îÄ Universal Semantic Chunker (universal_chunking.py)\r\n‚îú‚îÄ‚îÄ RAG Query Engine (rag_query.py)\r\n‚îú‚îÄ‚îÄ Collection Management (collection_manifest.py)\r\n‚îî‚îÄ‚îÄ Calibre Integration (calibre_db.py)\r\n```\r\n\r\n**Component Responsibilities:**\r\n\r\n**Ingestion Engine:**\r\n- Extract text from EPUB/PDF/TXT files\r\n- Coordinate chunking and embedding\r\n- Upload to Qdrant with metadata\r\n- Log to manifest system\r\n\r\n**Universal Semantic Chunker:**\r\n- Split text by semantic topic boundaries\r\n- Use sentence embeddings + cosine similarity\r\n- Enforce min/max chunk size constraints\r\n- Domain-agnostic approach\r\n\r\n**RAG Query Engine:**\r\n- Embed user query\r\n- Search Qdrant with filters\r\n- Apply similarity threshold\r\n- Optional LLM reranking\r\n- Generate answers via OpenRouter\r\n\r\n**Collection Management:**\r\n- Track ingested books per collection\r\n- Export manifests to JSON/CSV\r\n- Auto-reset on collection deletion\r\n- Resume interrupted batch ingestion\r\n\r\n**Calibre Integration:**\r\n- Direct SQLite access to metadata.db\r\n- Browse library with filters\r\n- Provide file paths for ingestion\r\n- Library statistics\r\n\r\n**See:** [docs/architecture/c4/03-component.md](c4/03-component.md)\r\n\r\n---\r\n\r\n## Data Flows\r\n\r\n### Ingestion Flow\r\n\r\n```\r\n1. User selects book in GUI\r\n2. GUI ‚Üí Ingestion Engine (file path + domain)\r\n3. Ingestion Engine ‚Üí Text Extractor (EPUB/PDF/TXT)\r\n4. Text Extractor ‚Üí Universal Semantic Chunker (raw text)\r\n5. Chunker ‚Üí Embedder (sentence embeddings for similarity)\r\n6. Chunker splits at semantic boundaries (threshold-based)\r\n7. Embedder ‚Üí Qdrant Uploader (chunk embeddings)\r\n8. Uploader ‚Üí Qdrant (batch upsert with metadata)\r\n9. Uploader ‚Üí Collection Management (log to manifest)\r\n```\r\n\r\n**Key Insight:** Chunks are created by semantic similarity, not word count. This preserves conceptual integrity.\r\n\r\n---\r\n\r\n### Query Flow\r\n\r\n```\r\n1. User enters query in GUI\r\n2. GUI ‚Üí RAG Query Engine (query + parameters)\r\n3. Query Engine ‚Üí Embedder (embed query)\r\n4. Query Engine ‚Üí Qdrant (search with filters)\r\n5. Qdrant ‚Üí Query Engine (top K chunks)\r\n6. Query Engine filters by similarity threshold\r\n7. (Optional) Query Engine ‚Üí OpenRouter (LLM reranking)\r\n8. Query Engine ‚Üí OpenRouter (generate answer with context)\r\n9. OpenRouter ‚Üí Query Engine (natural language answer)\r\n10. Query Engine ‚Üí GUI (answer + source citations)\r\n```\r\n\r\n**Key Insight:** Threshold filtering removes irrelevant results before LLM sees them, improving answer quality.\r\n\r\n---\r\n\r\n## Key Technologies\r\n\r\n| Component | Technology | Purpose |\r\n|-----------|-----------|---------|\r\n| GUI | Streamlit | Web interface |\r\n| Backend | Python 3.14 | All business logic |\r\n| Vector DB | Qdrant | Semantic search (cosine distance) |\r\n| Embeddings | sentence-transformers (all-MiniLM-L6-v2) | 384-dim vectors |\r\n| LLM API | OpenRouter | Multi-model inference |\r\n| Book Metadata | Calibre (SQLite) | Library management |\r\n| Text Extraction | PyMuPDF, ebooklib | PDF/EPUB parsing |\r\n\r\n---\r\n\r\n## Universal Semantic Chunking\r\n\r\n**Core Algorithm:**\r\n\r\n```python\r\n1. Split text into sentences (regex)\r\n2. Embed all sentences (all-MiniLM-L6-v2)\r\n3. For each sentence pair:\r\n   - Calculate cosine similarity\r\n   - If similarity < threshold AND buffer >= min_size:\r\n     ‚Üí Create chunk\r\n   - Else:\r\n     ‚Üí Add to buffer\r\n4. Enforce max_chunk_size safety cap\r\n```\r\n\r\n**Parameters:**\r\n- **Threshold:** 0.55 (default), 0.45 (philosophy) - Lower = more splits\r\n- **Min chunk size:** 200 words - Prevents atomic chunks\r\n- **Max chunk size:** 1200 words - Safety cap for LLM context\r\n\r\n**Benefits:**\r\n- ‚úÖ Semantic integrity (breaks at topic boundaries)\r\n- ‚úÖ Domain-agnostic (same logic for all content)\r\n- ‚úÖ Adaptive (automatically adjusts to content structure)\r\n- ‚úÖ Context preservation (minimum buffer ensures LLM has context)\r\n\r\n**See:** [docs/architecture/technical/UNIVERSAL_SEMANTIC_CHUNKING.md](technical/UNIVERSAL_SEMANTIC_CHUNKING.md)\r\n\r\n---\r\n\r\n## Architecture Principles\r\n\r\n### 1. Scripts-First Architecture\r\n\r\n**Principle:** All business logic lives in `scripts/` package. GUI is a thin presentation layer.\r\n\r\n**Why?**\r\n- Single source of truth (no duplication)\r\n- Multiple interfaces (GUI, CLI, AI agents)\r\n- Easy testing (no GUI overhead)\r\n- Clear separation of concerns\r\n\r\n**See:** [ADR 0003: GUI as Thin Layer](decisions/0003-gui-as-thin-layer.md)\r\n\r\n---\r\n\r\n### 2. Collection Isolation\r\n\r\n**Principle:** Each collection has separate manifests, progress files, and can use different settings.\r\n\r\n**Why?**\r\n- Prevents cross-contamination\r\n- Allows experimentation (test different chunking strategies)\r\n- Supports multiple use cases (personal library, research corpus, client project)\r\n\r\n**Files:**\r\n- `logs/{collection}_manifest.json` - Master manifest\r\n- `logs/{collection}_manifest.csv` - Human-readable export\r\n- `scripts/batch_ingest_progress_{collection}.json` - Resume tracker\r\n\r\n**See:** [ADR 0004: Collection-Specific Manifests](decisions/0004-collection-specific-manifests.md)\r\n\r\n---\r\n\r\n### 3. Progressive Enhancement\r\n\r\n**Principle:** Core functionality works with minimal dependencies. Advanced features are optional.\r\n\r\n**Examples:**\r\n- Ingestion works without Calibre DB (use folder ingestion)\r\n- Query works without OpenRouter (search-only mode, no answer generation)\r\n- GUI is optional (CLI works standalone)\r\n\r\n**Why?**\r\n- Easier onboarding (start simple, add features as needed)\r\n- Resilience (system degrades gracefully)\r\n- Flexibility (choose features based on use case)\r\n\r\n---\r\n\r\n## Deployment Architecture\r\n\r\n### Current: Single-User Desktop\r\n\r\n```\r\n[User's PC]\r\n‚îú‚îÄ‚îÄ Alexandria (Python scripts + Streamlit GUI)\r\n‚îú‚îÄ‚îÄ Calibre Library (SQLite)\r\n‚îî‚îÄ‚îÄ Docker Desktop\r\n    ‚îî‚îÄ‚îÄ Qdrant Container (localhost:6333)\r\n```\r\n\r\n**Access:** http://localhost:8501\r\n\r\n---\r\n\r\n### Future: NAS Deployment (Planned)\r\n\r\n```\r\n[NAS - 192.168.0.151]\r\n‚îú‚îÄ‚îÄ Docker: Alexandria Container\r\n‚îÇ   ‚îú‚îÄ‚îÄ Port: 8501 (Streamlit)\r\n‚îÇ   ‚îî‚îÄ‚îÄ Volumes: /books, /calibre, /logs\r\n‚îú‚îÄ‚îÄ Docker: Qdrant Container\r\n‚îÇ   ‚îî‚îÄ‚îÄ Port: 6333\r\n‚îú‚îÄ‚îÄ Calibre Library (metadata.db)\r\n‚îî‚îÄ‚îÄ Book Storage (EPUB/PDF files)\r\n```\r\n\r\n**Benefits:**\r\n- 24/7 availability\r\n- Multi-device access (phone, tablet, desktop)\r\n- Centralized storage with RAID backup\r\n- Low latency (Alexandria and Qdrant on same host)\r\n\r\n**See:** [docs/architecture/c4/02-container.md](c4/02-container.md) - Deployment section\r\n\r\n---\r\n\r\n## Security\r\n\r\n### Current Posture (Single-User)\r\n\r\n- **Qdrant:** No authentication (local network only)\r\n- **OpenRouter:** API key in `.streamlit/secrets.toml` (gitignored)\r\n- **Calibre DB:** Read-only access\r\n- **File System:** Local user permissions\r\n\r\n### Future Hardening (Multi-User)\r\n\r\n- Qdrant authentication (API key or JWT)\r\n- User authentication in GUI (Streamlit auth)\r\n- Role-based access control (read-only vs admin)\r\n- API rate limiting\r\n\r\n---\r\n\r\n## Performance Characteristics\r\n\r\n### Ingestion Throughput\r\n\r\n**Benchmark (typical book, ~500 pages):**\r\n- Text extraction: ~5 seconds\r\n- Semantic chunking: ~3-5 seconds\r\n- Embedding generation: ~2 seconds\r\n- Qdrant upload: ~1 second\r\n- **Total:** ~11-13 seconds per book\r\n\r\n**Bottleneck:** Semantic chunking (6x slower than fixed-window, but better quality)\r\n\r\n---\r\n\r\n### Query Latency\r\n\r\n**Benchmark (typical query):**\r\n- Query embedding: ~0.1 seconds\r\n- Qdrant search: ~0.3 seconds\r\n- LLM answer generation: ~2-5 seconds (depends on model)\r\n- **Total:** ~2.5-5.5 seconds\r\n\r\n**Bottleneck:** LLM inference (OpenRouter API network latency)\r\n\r\n---\r\n\r\n### Scalability\r\n\r\n**Current Scale:**\r\n- ~150 books ingested\r\n- ~23,000 chunks in Qdrant\r\n- ~9,000 books in Calibre library (not all ingested)\r\n\r\n**Projected Scale (full library):**\r\n- 9,000 books √ó ~150 chunks/book = ~1.35 million chunks\r\n- 1.35M chunks √ó 384 dims √ó 4 bytes = ~2 GB vectors\r\n- Qdrant easily handles this on commodity hardware\r\n\r\n**Future Scale (if needed):**\r\n- Qdrant supports billions of vectors\r\n- Can add multiple Qdrant nodes (clustering)\r\n- Can partition by domain (separate collections)\r\n\r\n---\r\n\r\n## Key Architecture Decisions (ADRs)\r\n\r\nAll architectural decisions are documented as ADRs. See [docs/architecture/decisions/README.md](decisions/README.md) for full index.\r\n\r\n**Implemented Decisions:**\r\n\r\n| ADR | Decision | Status | Impact |\r\n|-----|----------|--------|--------|\r\n| [0001](decisions/0001-use-qdrant-vector-db.md) | Use Qdrant Vector DB | ‚úÖ Implemented | Fast semantic search (<100ms) |\r\n| [0002](decisions/0002-domain-specific-chunking.md) | Domain-Specific Chunking | ‚ö†Ô∏è Superseded | Replaced by Universal Semantic |\r\n| [0003](decisions/0003-gui-as-thin-layer.md) | GUI as Thin Presentation Layer | ‚úÖ Implemented | Reduced 160 LOC duplication |\r\n| [0004](decisions/0004-collection-specific-manifests.md) | Collection-Specific Manifests | ‚úÖ Implemented | Prevents cross-contamination |\r\n| [0005](decisions/0005-philosophical-argument-chunking.md) | Philosophical Argument Chunking | ‚ö†Ô∏è Superseded | Incorporated into Universal Semantic |\r\n| [0006](decisions/0006-separate-systems-architecture.md) | Separate Systems Architecture | ‚úÖ Implemented | Clear separation of concerns |\r\n\r\n---\r\n\r\n## Related Documentation\r\n\r\n### Architecture Docs\r\n\r\n- **[Architecture Overview](README.md)** - Navigation hub\r\n- **[C4 Context](c4/01-context.md)** - System overview\r\n- **[C4 Container](c4/02-container.md)** - Major components\r\n- **[C4 Component](c4/03-component.md)** - Internal structure\r\n- **[Structurizr Workspace](workspace.dsl)** - Interactive diagrams\r\n\r\n### Technical Specs\r\n\r\n- **[Universal Semantic Chunking](technical/UNIVERSAL_SEMANTIC_CHUNKING.md)** - Chunking deep-dive\r\n- **[Qdrant Payload Structure](technical/QDRANT_PAYLOAD_STRUCTURE.md)** - Vector DB schema\r\n- **[PDF vs EPUB Comparison](technical/PDF_vs_EPUB_COMPARISON.md)** - Format analysis\r\n\r\n### User Guides\r\n\r\n- **[Quick Reference](../guides/QUICK_REFERENCE.md)** - Command cheat sheet\r\n- **[Logging Guide](../guides/LOGGING_GUIDE.md)** - Tracking system\r\n- **[Professional Setup](../guides/PROFESSIONAL_SETUP_COMPLETE.md)** - Complete guide\r\n\r\n---\r\n\r\n## View Interactive Diagrams\r\n\r\n### Structurizr Lite (Recommended)\r\n\r\n```bash\r\n# Start Structurizr Lite on port 8081\r\ncd docs/architecture\r\ndocker run -it --rm -p 8081:8080 -v \"%cd%:/usr/local/structurizr\" structurizr/lite\r\n```\r\n\r\nOpen: http://localhost:8081\r\n\r\n**Views Available:**\r\n- System Context - High-level ecosystem\r\n- Containers - Major components\r\n- Components - Internal structure\r\n- Detailed Ingestion Flow - Book processing pipeline\r\n\r\n---\r\n\r\n## Contact & Contributions\r\n\r\n**Project:** Alexandria of Temenos\r\n**Status:** Phase 1 - Production Ready ‚úÖ\r\n**License:** Internal Use (Temenos Academy)\r\n\r\n**For questions or contributions:**\r\n- See [docs/stories/](../stories/) for feature documentation\r\n- See [docs/architecture/decisions/](decisions/) for ADRs\r\n- See [TODO.md](../../TODO.md) for backlog\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-25\r\n**Version:** 1.0 (Universal Semantic Chunking)\r\n",
      "format" : "Markdown",
      "order" : 1,
      "title" : ""
    }, {
      "content" : "# C4 Level 1: System Context\r\n\r\n**Purpose:** Shows Alexandria RAG System in its broader ecosystem - who uses it and what external systems it depends on.\r\n\r\n---\r\n\r\n## Diagram\r\n\r\nView interactively: http://localhost:8081 ‚Üí \"SystemContext\" view\r\n\r\nOr see `workspace.dsl` lines defining the context.\r\n\r\n---\r\n\r\n## System Overview\r\n\r\n**Alexandria RAG System** is a Retrieval-Augmented Generation (RAG) system that enables semantic search and knowledge synthesis across a multi-disciplinary library of ~9,000 books.\r\n\r\n### Users\r\n\r\n**Developer/Researcher**\r\n- Uses Alexandria to search and analyze books\r\n- Seeks cross-domain insights (e.g., \"manufacturing patterns in 18th-century textile mills\")\r\n- Ingests new books into the system\r\n- Queries for answers grounded in book content\r\n\r\n### External Systems\r\n\r\n**Qdrant Vector DB (192.168.0.151:6333)**\r\n- **Purpose:** Stores 384-dimensional embeddings of book chunks\r\n- **What it provides:** Fast semantic similarity search\r\n- **Data stored:** ~153 chunks per book, metadata (domain, author, title, chunk_index)\r\n- **Why external:** Specialized vector search engine, scales to millions of vectors\r\n\r\n**OpenRouter API**\r\n- **Purpose:** LLM inference for natural language answer generation\r\n- **What it provides:** Multiple models (GPT-4, Claude, etc.) via unified API\r\n- **Integration:** Receives retrieved chunks + user query ‚Üí Returns coherent answer\r\n- **Why external:** Provides access to state-of-the-art LLMs without local hosting\r\n\r\n---\r\n\r\n## Information Flow\r\n\r\n### Ingestion Flow\r\n```\r\nUser ‚Üí Alexandria ‚Üí File System (reads books) ‚Üí Alexandria (chunks + embeds) ‚Üí Qdrant (stores)\r\n```\r\n\r\n### Query Flow\r\n```\r\nUser ‚Üí Alexandria ‚Üí Qdrant (semantic search) ‚Üí Alexandria ‚Üí OpenRouter (answer generation) ‚Üí User\r\n```\r\n\r\n---\r\n\r\n## Scope & Boundaries\r\n\r\n### In Scope\r\n- Book ingestion (EPUB, PDF, TXT, MD)\r\n- Domain-specific chunking\r\n- Semantic search via Qdrant\r\n- LLM-powered answer generation\r\n- Manifest tracking\r\n\r\n### Out of Scope\r\n- Book format conversion (use Calibre)\r\n- LLM model hosting (use OpenRouter API)\r\n- Vector DB hosting (use Qdrant server)\r\n- Image/diagram processing (text-only)\r\n\r\n---\r\n\r\n## Design Drivers\r\n\r\n### Why RAG?\r\n- **Grounding:** LLM answers are backed by actual book content (prevents hallucination)\r\n- **Citations:** Every answer includes source book + chunk references\r\n- **Freshness:** New books instantly available (no model retraining)\r\n- **Explainability:** Can trace answer back to specific passages\r\n\r\n### Why 9,000 books?\r\n- **Multi-disciplinary synthesis:** Technical + Psychology + Philosophy + History\r\n- **Cross-domain insights:** \"How do psychological principles apply to UX design?\"\r\n- **Gap awareness:** System knows what it doesn't know (missing books tracked)\r\n\r\n### Why Qdrant?\r\n- **Performance:** Sub-second search across millions of vectors\r\n- **Flexibility:** Metadata filtering (domain, author, date)\r\n- **Scalability:** Can grow to full library (9,383 books)\r\n\r\n---\r\n\r\n## Integration Points\r\n\r\n| System | Protocol | Purpose |\r\n|--------|----------|---------|\r\n| Qdrant | HTTP/gRPC (Python client) | Store/retrieve embeddings |\r\n| OpenRouter | REST API | Generate answers from context |\r\n| Calibre | Direct SQLite read | Extract book metadata |\r\n| File System | Local disk I/O | Read books, write logs |\r\n\r\n---\r\n\r\n## Related Views\r\n\r\n- **Next Level:** [Container Diagram](02-container.md) - Internal structure of Alexandria\r\n- **Related:** [ADR 0001: Use Qdrant Vector DB](../decisions/0001-use-qdrant-vector-db.md)\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-23\r\n",
      "filename" : "01-context.md",
      "format" : "Markdown",
      "order" : 2,
      "title" : ""
    }, {
      "content" : "# C4 Level 2: Container Diagram\r\n\r\n**Purpose:** Shows the major architectural components (containers) inside Alexandria RAG System and how they interact.\r\n\r\n---\r\n\r\n## Diagram\r\n\r\nView interactively: http://localhost:8081 ‚Üí \"Containers\" view\r\n\r\nOr see `workspace.dsl` container definitions.\r\n\r\n---\r\n\r\n## Containers\r\n\r\n### 1. Streamlit GUI (Web Browser)\r\n**Technology:** Python 3.14, Streamlit\r\n**Purpose:** Web-based interface for all user interactions\r\n\r\n**Responsibilities:**\r\n- Browse Calibre library with filters\r\n- Select books for ingestion with domain assignment\r\n- Execute RAG queries with parameter controls\r\n- Display collection statistics and manifests\r\n- Manage OpenRouter API configuration\r\n\r\n**Key Features:**\r\n- Purple gradient theme with professional layout\r\n- Session state management for interactive features\r\n- Real-time parameter validation\r\n- CSV export functionality\r\n\r\n**Architecture Principle:** **Thin presentation layer** - no business logic, only UI and user input handling.\r\n\r\n**Related Story:** [04-GUI.md](../../stories/04-GUI.md)\r\n\r\n---\r\n\r\n### 2. Scripts Package (Python Modules)\r\n**Technology:** Python 3.14\r\n**Purpose:** Core business logic for all operations\r\n\r\n**Responsibilities:**\r\n- Book ingestion and processing\r\n- Domain-specific chunking\r\n- Semantic search and RAG queries\r\n- Collection management and manifest tracking\r\n- Calibre database integration\r\n\r\n**Why Separate from GUI?**\r\n- **CLI support:** Can be called from command line\r\n- **AI agent support:** Agents call functions directly\r\n- **Testing:** Unit tests without GUI overhead\r\n- **Single source of truth:** One implementation for all interfaces\r\n\r\n**Internal Components:** See [Component Diagram](03-component.md)\r\n\r\n**Related ADR:** [ADR 0003: GUI as Thin Layer](../decisions/0003-gui-as-thin-layer.md)\r\n\r\n---\r\n\r\n### 3. File System (Storage)\r\n**Technology:** Local disk (Windows file system)\r\n**Purpose:** Book storage and logging\r\n\r\n**Directory Structure:**\r\n```\r\nAlexandria/\r\n‚îú‚îÄ‚îÄ ingest/         # Books waiting to be processed\r\n‚îú‚îÄ‚îÄ ingested/       # Successfully processed books (archive)\r\n‚îî‚îÄ‚îÄ logs/           # Manifest JSON/CSV files\r\n```\r\n\r\n**What's Stored:**\r\n- **Books:** EPUB, PDF, TXT, MD files\r\n- **Manifests:** Per-collection JSON manifests\r\n- **Progress:** Batch ingestion resume files\r\n- **Exports:** CSV exports of manifests\r\n\r\n**Related Story:** [06-COLLECTION_MANAGEMENT.md](../../stories/06-COLLECTION_MANAGEMENT.md)\r\n\r\n---\r\n\r\n### 4. Calibre Database (SQLite)\r\n**Technology:** SQLite (metadata.db)\r\n**Purpose:** Book metadata storage\r\n\r\n**Location:** Calibre library folder (typically `Documents/Calibre Library`)\r\n\r\n**Schema (relevant tables):**\r\n- `books` - Book records (id, title, sort, timestamp)\r\n- `authors` - Author names\r\n- `data` - File paths and formats\r\n- `comments` - Descriptions\r\n- `identifiers` - ISBN, etc.\r\n- `languages` - Language codes\r\n- `tags` - User-defined tags\r\n- `series` - Series information\r\n\r\n**Access Pattern:** Read-only via direct SQLite queries (no Calibre API)\r\n\r\n**Related Story:** [05-CALIBRE_INTEGRATION.md](../../stories/05-CALIBRE_INTEGRATION.md)\r\n\r\n---\r\n\r\n## Container Interactions\r\n\r\n### Ingestion Flow\r\n```\r\nGUI ‚Üí Scripts Package ‚Üí File System (read book)\r\nScripts Package ‚Üí Calibre DB (get metadata)\r\nScripts Package ‚Üí Qdrant (upload chunks)\r\nScripts Package ‚Üí File System (write manifest)\r\n```\r\n\r\n### Query Flow\r\n```\r\nGUI ‚Üí Scripts Package ‚Üí Qdrant (search)\r\nScripts Package ‚Üí OpenRouter (generate answer)\r\nScripts Package ‚Üí GUI (return result)\r\n```\r\n\r\n### Browse Flow\r\n```\r\nGUI ‚Üí Scripts Package ‚Üí Calibre DB (query books)\r\nScripts Package ‚Üí GUI (return book list)\r\n```\r\n\r\n---\r\n\r\n## Technology Choices\r\n\r\n### Why Streamlit?\r\n- **Rapid development:** Professional UI in ~500 LOC\r\n- **Python native:** No separate frontend framework\r\n- **Interactive widgets:** Built-in session state management\r\n- **Deployment:** Easy to share via cloud or localhost\r\n\r\n### Why Python Scripts Package?\r\n- **Single language:** Python for everything (no polyglot complexity)\r\n- **Rich ecosystem:** sentence-transformers, qdrant-client, ebooklib\r\n- **AI-friendly:** Easy for AI agents to read and modify\r\n- **XAF parallel:** Similar to WBF2's module architecture (business logic separate from UI)\r\n\r\n### Why SQLite for Calibre?\r\n- **No dependencies:** Direct file access (no Calibre server needed)\r\n- **Fast:** Indexed queries return 9,000 books in <2 seconds\r\n- **Standard:** Calibre uses SQLite for all metadata\r\n\r\n---\r\n\r\n## Deployment Model\r\n\r\n### Current: Single-User Desktop\r\n```\r\n[User's PC]\r\n  ‚îú‚îÄ‚îÄ Alexandria (Python scripts + GUI)\r\n  ‚îú‚îÄ‚îÄ Calibre Library (SQLite)\r\n  ‚îî‚îÄ‚îÄ Docker Desktop\r\n        ‚îî‚îÄ‚îÄ Qdrant (vector DB)\r\n```\r\n\r\n### Future: NAS Deployment (Docker)\r\n```\r\n[NAS - 192.168.0.151]\r\n  ‚îú‚îÄ‚îÄ Docker: Alexandria Container (GUI + Scripts)\r\n  ‚îÇ   ‚îú‚îÄ‚îÄ Port: 8501 (Streamlit)\r\n  ‚îÇ   ‚îú‚îÄ‚îÄ Volume: /nas/books ‚Üí /app/ingest\r\n  ‚îÇ   ‚îú‚îÄ‚îÄ Volume: /nas/calibre ‚Üí /app/calibre\r\n  ‚îÇ   ‚îî‚îÄ‚îÄ Secrets: .streamlit/secrets.toml\r\n  ‚îú‚îÄ‚îÄ Docker: Qdrant (already running)\r\n  ‚îÇ   ‚îî‚îÄ‚îÄ Port: 6333\r\n  ‚îú‚îÄ‚îÄ Calibre Library (metadata.db)\r\n  ‚îî‚îÄ‚îÄ Book Storage (EPUB/PDF files)\r\n```\r\n\r\n**Benefits:**\r\n- Always-on access (24/7 availability)\r\n- Multi-device support (phone, tablet, desktop)\r\n- Centralized storage with RAID backup\r\n- Low latency to Qdrant (same host)\r\n\r\n**Deployment:**\r\n```dockerfile\r\nFROM python:3.14\r\nWORKDIR /app\r\nCOPY requirements.txt .\r\nRUN pip install -r requirements.txt\r\nCOPY . .\r\nEXPOSE 8501\r\nCMD [\"streamlit\", \"run\", \"alexandria_app.py\", \\\r\n     \"--server.address\", \"0.0.0.0\"]\r\n```\r\n\r\n**When to Deploy:**\r\n- GUI stable (no frequent code changes)\r\n- Daily usage pattern established\r\n- Need multi-device access\r\n\r\n**Status:** Planned (see [TODO.md - Backlog #6](../../../TODO.md))\r\n\r\n### Future: Multi-User Server (Cloud - Alternative)\r\n```\r\n[Web Server]\r\n  ‚îú‚îÄ‚îÄ Alexandria GUI (Streamlit Cloud)\r\n  ‚îî‚îÄ‚îÄ Alexandria Scripts (Python backend)\r\n\r\n[Vector Server]\r\n  ‚îî‚îÄ‚îÄ Qdrant (cloud or self-hosted)\r\n\r\n[LLM API]\r\n  ‚îî‚îÄ‚îÄ OpenRouter (cloud)\r\n```\r\n\r\n**Note:** NAS deployment preferred over cloud for privacy and cost.\r\n\r\n---\r\n\r\n## Security Considerations\r\n\r\n### Current Security Posture\r\n- **Qdrant:** No authentication (local network 192.168.0.151:6333)\r\n- **OpenRouter:** API key in `.streamlit/secrets.toml` (gitignored)\r\n- **Calibre DB:** Read-only access (no writes)\r\n- **File System:** Local user permissions\r\n\r\n### Future Hardening (if multi-user)\r\n- Qdrant authentication\r\n- User authentication in GUI\r\n- Role-based access control\r\n- API rate limiting\r\n\r\n---\r\n\r\n## Related Views\r\n\r\n- **Previous Level:** [System Context](01-context.md)\r\n- **Next Level:** [Component Diagram](03-component.md)\r\n- **Related ADR:** [ADR 0003: GUI as Thin Layer](../decisions/0003-gui-as-thin-layer.md)\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-23\r\n",
      "filename" : "02-container.md",
      "format" : "Markdown",
      "order" : 3,
      "title" : ""
    }, {
      "content" : "# C4 Level 3: Component Diagram (Scripts Package)\r\n\r\n**Purpose:** Shows the internal structure of the Scripts Package container - the core business logic components.\r\n\r\n---\r\n\r\n## Diagram\r\n\r\nView interactively: http://localhost:8081 ‚Üí \"Components\" view\r\n\r\nOr see `workspace.dsl` component definitions.\r\n\r\n---\r\n\r\n## Components\r\n\r\n### 1. Ingestion Engine\r\n**Files:** `batch_ingest.py`, `ingest_books.py`\r\n**Purpose:** Processes books into chunks and uploads to Qdrant\r\n\r\n**Responsibilities:**\r\n- Extract text from EPUB/PDF/TXT/MD files\r\n- Apply domain-specific chunking strategies\r\n- Generate embeddings (sentence-transformers)\r\n- Upload chunks to Qdrant with metadata\r\n- Log to manifest system\r\n- Resume interrupted batch ingestion\r\n\r\n**Key Functions:**\r\n- `ingest_book()` - Single book ingestion\r\n- `batch_ingest()` - Multiple books with resume\r\n- `extract_text_from_epub()` - EPUB text extraction\r\n- `extract_text_from_pdf()` - PDF text extraction\r\n- `create_chunks_from_sections()` - Chunking orchestrator\r\n\r\n**Integration Points:**\r\n- ‚Üí Chunking Strategies (applies chunking)\r\n- ‚Üí Collection Management (logs manifest)\r\n- ‚Üí File System (reads books, moves to ingested/)\r\n- ‚Üí Qdrant (uploads chunks)\r\n\r\n**Related Story:** [01-INGESTION.md](../../stories/01-INGESTION.md)\r\n\r\n---\r\n\r\n### 2. Universal Semantic Chunker\r\n**Files:** `universal_chunking.py`, `ingest_books.py`\r\n**Purpose:** Semantic-aware text chunking for optimal retrieval across all domains\r\n\r\n**Responsibilities:**\r\n- Split text into sentences using regex\r\n- Generate embeddings for all sentences\r\n- Calculate cosine similarity between consecutive sentences\r\n- Break chunks at semantic topic boundaries (low similarity)\r\n- Enforce min/max chunk size constraints\r\n- Domain-agnostic approach (works for all content types)\r\n\r\n**Algorithm:**\r\n```\r\n1. Split text into sentences\r\n2. Embed all sentences (all-MiniLM-L6-v2)\r\n3. For each sentence pair:\r\n   - Calculate similarity with previous sentence\r\n   - If similarity < threshold AND buffer >= min_size:\r\n     ‚Üí Create new chunk\r\n   - Else: Add to current buffer\r\n4. Enforce max_chunk_size safety cap\r\n```\r\n\r\n**Parameters:**\r\n\r\n| Parameter | Default | Philosophy | Description |\r\n|-----------|---------|------------|-------------|\r\n| threshold | 0.55 | 0.45 | Lower = fewer breaks, larger chunks |\r\n| min_chunk_size | 200 words | 200 words | Minimum context buffer |\r\n| max_chunk_size | 1200 words | 1200 words | Safety cap for LLM limits |\r\n\r\n**Philosophy Tuning:**\r\n- Lower threshold (0.45) for tighter topic focus\r\n- Preserves argument coherence through semantic similarity\r\n- No hard-coded opposition detection needed\r\n\r\n**Key Functions:**\r\n- `UniversalChunker.chunk()` - Main chunking entry point\r\n- `_split_sentences()` - Regex-based sentence splitting\r\n- `_create_chunk_dict()` - Chunk metadata generation\r\n\r\n**Benefits:**\r\n- **Semantic integrity:** Breaks where topic changes, not at word count\r\n- **Domain agnostic:** Same logic for technical, psychology, philosophy, literature\r\n- **Adaptive:** Automatically adjusts to content structure\r\n- **Context preservation:** Maintains minimum chunk size for LLM context\r\n\r\n**Related Story:** [02-CHUNKING.md](../../stories/02-CHUNKING.md)\r\n**Related ADR:** [ADR 0002: Domain-Specific Chunking](../decisions/0002-domain-specific-chunking.md) - Superseded by Universal Semantic approach\r\n\r\n---\r\n\r\n### 3. RAG Query Engine\r\n**Files:** `rag_query.py`\r\n**Purpose:** Semantic search with LLM answer generation\r\n\r\n**Responsibilities:**\r\n- Execute semantic search against Qdrant\r\n- Apply similarity threshold filtering\r\n- Fetch multiplier control (quality vs speed)\r\n- Optional LLM reranking\r\n- Generate natural language answers via OpenRouter\r\n- Format results with source citations\r\n\r\n**Query Pipeline:**\r\n```\r\n1. Embed query text (sentence-transformers)\r\n2. Search Qdrant (fetch_multiplier √ó limit results)\r\n3. Filter by similarity threshold (default: 0.3)\r\n4. Optionally rerank with LLM\r\n5. Send top chunks to OpenRouter\r\n6. Generate answer with citations\r\n7. Return RAGResult (answer, sources, metadata)\r\n```\r\n\r\n**Key Functions:**\r\n- `perform_rag_query()` - Main entry point (CLI + GUI + agents)\r\n- `search_qdrant()` - Semantic search with filters\r\n- `generate_answer()` - OpenRouter API integration\r\n- `format_context()` - Prepare chunks for LLM\r\n\r\n**Configuration:**\r\n- `fetch_multiplier` (default: 3) - How many extra results to fetch\r\n- `similarity_threshold` (default: 0.3) - Minimum relevance score\r\n- `temperature` (default: 0.7) - LLM creativity control\r\n- `reranking` (default: False) - Enable LLM reranking\r\n\r\n**Integration Points:**\r\n- ‚Üí Collection Management (verify collection exists)\r\n- ‚Üí Qdrant (semantic search)\r\n- ‚Üí OpenRouter (answer generation)\r\n\r\n**Related Story:** [03-RAG_QUERY.md](../../stories/03-RAG_QUERY.md)\r\n\r\n---\r\n\r\n### 4. Collection Management\r\n**Files:** `collection_manifest.py`, `qdrant_utils.py`\r\n**Purpose:** Track ingested books and manage Qdrant collections\r\n\r\n**Responsibilities:**\r\n- Maintain per-collection manifests (JSON/CSV)\r\n- Track ingestion progress for resume\r\n- Verify collection exists in Qdrant\r\n- Collection operations (stats, search, copy, delete)\r\n- Auto-reset manifests when collection deleted\r\n- CSV export for human-readable reports\r\n\r\n**Manifest Structure:**\r\n```json\r\n{\r\n  \"created_at\": \"2026-01-23T...\",\r\n  \"last_updated\": \"2026-01-23T...\",\r\n  \"books\": [\r\n    {\r\n      \"file_path\": \"c:/path/to/book.epub\",\r\n      \"book_title\": \"Title\",\r\n      \"author\": \"Author\",\r\n      \"domain\": \"technical\",\r\n      \"language\": \"ENG\",\r\n      \"file_type\": \"EPUB\",\r\n      \"chunks_count\": 153,\r\n      \"file_size_mb\": 34.2,\r\n      \"ingested_at\": \"2026-01-23T...\"\r\n    }\r\n  ],\r\n  \"total_chunks\": 153,\r\n  \"total_size_mb\": 34.2\r\n}\r\n```\r\n\r\n**Key Functions:**\r\n- `log_book()` - Add book to manifest\r\n- `is_book_ingested()` - Check if already processed\r\n- `show_manifest()` - Display collection contents\r\n- `export_to_csv()` - Human-readable export\r\n- `verify_collection_exists()` - Auto-reset on deletion\r\n\r\n**Files:**\r\n- `logs/{collection_name}_manifest.json` - Master manifest\r\n- `logs/{collection_name}_manifest.csv` - CSV export\r\n- `scripts/batch_ingest_progress_{collection_name}.json` - Resume tracker\r\n\r\n**Integration Points:**\r\n- ‚Üê Ingestion Engine (logs new books)\r\n- ‚Üê RAG Query Engine (checks collection status)\r\n- ‚Üí File System (reads/writes manifests)\r\n- ‚Üí Qdrant (collection operations)\r\n\r\n**Related Story:** [06-COLLECTION_MANAGEMENT.md](../../stories/06-COLLECTION_MANAGEMENT.md)\r\n**Related ADR:** [ADR 0004: Collection-Specific Manifests](../decisions/0004-collection-specific-manifests.md)\r\n\r\n---\r\n\r\n### 5. Calibre Integration\r\n**Files:** `calibre_db.py`\r\n**Purpose:** Direct SQLite access to Calibre library\r\n\r\n**Responsibilities:**\r\n- Query Calibre metadata.db directly\r\n- Extract book metadata (title, author, series, tags, languages)\r\n- Match files to books (fuzzy matching)\r\n- Provide file paths for direct ingestion\r\n- Library statistics (books, authors, formats, languages)\r\n\r\n**CalibreBook Dataclass:**\r\n```python\r\n@dataclass\r\nclass CalibreBook:\r\n    id: int\r\n    title: str\r\n    authors: List[str]\r\n    formats: List[str]  # ['EPUB', 'PDF', 'MOBI']\r\n    file_paths: List[str]\r\n    languages: List[str]  # ISO codes\r\n    tags: List[str]\r\n    series: Optional[str]\r\n    series_index: Optional[float]\r\n    isbn: Optional[str]\r\n    rating: Optional[int]\r\n    timestamp: str\r\n```\r\n\r\n**Key Functions:**\r\n- `get_all_books()` - Load entire library (~9,000 books in <2s)\r\n- `search_books()` - Filter by author, title, language, format, tags\r\n- `match_file_to_book()` - Find book by file path\r\n- `get_stats()` - Library statistics\r\n\r\n**Integration Points:**\r\n- ‚Üê GUI (browse library)\r\n- ‚Üí Calibre Database (read-only queries)\r\n- ‚Üí Ingestion Engine (provides book paths)\r\n\r\n**Related Story:** [05-CALIBRE_INTEGRATION.md](../../stories/05-CALIBRE_INTEGRATION.md)\r\n\r\n---\r\n\r\n## Component Interactions\r\n\r\n### Ingestion Flow\r\n```\r\nCalibre Integration ‚Üí Ingestion Engine (book path + metadata)\r\nIngestion Engine ‚Üí Universal Semantic Chunker (split by semantic similarity)\r\nUniversal Semantic Chunker ‚Üí Embedder (generate chunk embeddings)\r\nIngestion Engine ‚Üí Collection Management (log to manifest)\r\nIngestion Engine ‚Üí Qdrant (upload chunks with metadata)\r\n```\r\n\r\n### Query Flow\r\n```\r\nRAG Query Engine ‚Üí Collection Management (verify collection)\r\nRAG Query Engine ‚Üí Qdrant (semantic search)\r\nRAG Query Engine ‚Üí OpenRouter (generate answer)\r\n```\r\n\r\n### Browse Flow\r\n```\r\nCalibre Integration ‚Üí Calibre Database (query books)\r\nCalibre Integration ‚Üí GUI (return book list)\r\n```\r\n\r\n---\r\n\r\n## Design Patterns\r\n\r\n### 1. **Module Pattern**\r\nEach component is a standalone Python module with clear interface.\r\n- Single entry point function (e.g., `perform_rag_query()`)\r\n- Usable by GUI, CLI, and AI agents\r\n- No circular dependencies\r\n\r\n### 2. **Configuration via Files**\r\n- `domains.json` - Domain list and chunking config\r\n- `{collection}_manifest.json` - Persistent state\r\n- `.streamlit/secrets.toml` - API keys\r\n\r\n### 3. **Progressive Enhancement**\r\n- Basic ingestion works without Calibre DB\r\n- RAG query works without OpenRouter (search-only mode)\r\n- GUI optional (CLI works standalone)\r\n\r\n---\r\n\r\n## Related Views\r\n\r\n- **Previous Level:** [Container Diagram](02-container.md)\r\n- **Stories:** [All feature stories](../../stories/)\r\n- **Code:** See `scripts/` directory for implementation\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-23\r\n",
      "filename" : "03-component.md",
      "format" : "Markdown",
      "order" : 4,
      "title" : ""
    }, {
      "content" : "# PDF vs EPUB Ingestion Comparison\r\n\r\n**Date:** 2026-01-21\r\n**Test Books:** Silverston Data Model Resource Book Vol 1 (PDF) vs Vol 3 (EPUB)\r\n\r\n---\r\n\r\n## Test Results\r\n\r\n### PDF Ingestion (Vol 1)\r\n- **Format:** PDF (3.4 MB)\r\n- **Pages:** 525\r\n- **Chunks Created:** 525 (1 chunk per page)\r\n- **Avg Chunk Size:** ~200 tokens/chunk\r\n- **Ingestion Time:** ~11 seconds (total)\r\n- **Embedding Time:** ~6 seconds (525 chunks)\r\n- **Upload Time:** ~1 second (6 batches)\r\n- **Search Quality:** 0.61-0.65 relevance scores ‚úÖ\r\n\r\n### EPUB Ingestion (Vol 3)\r\n- **Format:** EPUB (34.2 MB)\r\n- **Chapters:** 20\r\n- **Chunks Created:** 153\r\n- **Avg Chunk Size:** ~1450 tokens/chunk\r\n- **Ingestion Time:** ~14 seconds (total)\r\n- **Embedding Time:** ~3 seconds (153 chunks)\r\n- **Upload Time:** ~1 second (2 batches)\r\n- **Search Quality:** 0.38-0.64 relevance scores ‚úÖ\r\n\r\n---\r\n\r\n## Key Differences\r\n\r\n### Chunk Size Distribution\r\n\r\n| Metric | PDF (Vol 1) | EPUB (Vol 3) | Notes |\r\n|--------|-------------|--------------|-------|\r\n| Total Chunks | 525 | 153 | PDF creates 3.4x more chunks |\r\n| Avg Size | ~200 tokens | ~1450 tokens | EPUB chunks 7x larger |\r\n| Chunking Strategy | Page-based | Content-based | PDF = 1 page = 1 chunk |\r\n| Context Preservation | Lower | Higher | EPUB preserves paragraph flow |\r\n\r\n### Why the Difference?\r\n\r\n**PDF Extraction:**\r\n- PyMuPDF extracts text **page-by-page**\r\n- Each page becomes a separate text block\r\n- Page breaks interrupt content flow\r\n- Result: Smaller, page-bounded chunks\r\n\r\n**EPUB Extraction:**\r\n- EbookLib extracts text **chapter-by-chapter**\r\n- Chapters are continuous text blocks\r\n- No artificial page breaks\r\n- Result: Larger, semantically coherent chunks\r\n\r\n---\r\n\r\n## Search Quality Comparison\r\n\r\n### Test Query: \"What are the universal data model patterns for orders?\"\r\n\r\n#### PDF Results (Vol 1)\r\n```\r\nSource 1 (Score: 0.6557)\r\nSection: Page 119\r\nText: \"Ordering Products 109... Order and Order Items... more flexible structure...\"\r\n\r\nSource 2 (Score: 0.6498)\r\nSection: Page 116\r\nText: \"Standard order model... SUPPLIER related to PURCHASE ORDERS...\"\r\n\r\nSource 3 (Score: 0.6151)\r\nSection: Page 427\r\nText: \"Implementing the Universal Data Models 423...\"\r\n```\r\n\r\n#### EPUB Results (Vol 3)\r\n```\r\nSource 1 (Score: 0.6404)\r\nSection: Chapter 6\r\nText: \"...shipment lifecycle... from 'Shipment Planned' to 'Shipment Closed'...\"\r\n\r\n(Different book, different topic - but similar relevance scores)\r\n```\r\n\r\n**Conclusion:** Both formats achieve good relevance scores (0.6+), indicating effective semantic search regardless of chunk size.\r\n\r\n---\r\n\r\n## Pros & Cons\r\n\r\n### PDF Ingestion\r\n\r\n**Pros:**\r\n- ‚úÖ Works reliably (525 pages processed successfully)\r\n- ‚úÖ Good search quality (0.61-0.65 scores)\r\n- ‚úÖ Fast processing (~11 seconds for 525 pages)\r\n- ‚úÖ Each page is searchable independently\r\n\r\n**Cons:**\r\n- ‚ö†Ô∏è Small chunks (~200 tokens) may lack context\r\n- ‚ö†Ô∏è Page breaks can split paragraphs/tables\r\n- ‚ö†Ô∏è 3.4x more chunks = 3.4x more embedding costs\r\n- ‚ö†Ô∏è Headers/footers may create noise\r\n\r\n### EPUB Ingestion\r\n\r\n**Pros:**\r\n- ‚úÖ Large chunks (~1450 tokens) preserve context\r\n- ‚úÖ Semantically coherent (chapter-based)\r\n- ‚úÖ Fewer chunks = lower embedding costs\r\n- ‚úÖ No page break artifacts\r\n\r\n**Cons:**\r\n- ‚ö†Ô∏è Depends on EPUB structure quality\r\n- ‚ö†Ô∏è Some EPUBs poorly structured (one giant chapter)\r\n- ‚ö†Ô∏è Larger file sizes (34 MB vs 3.4 MB)\r\n\r\n---\r\n\r\n## Recommendations\r\n\r\n### When to Use Which Format?\r\n\r\n**Prefer EPUB if available:**\r\n- Better semantic coherence\r\n- Lower embedding costs (fewer chunks)\r\n- Cleaner text extraction\r\n\r\n**Use PDF if EPUB not available:**\r\n- Still produces good results\r\n- Widely available format\r\n- Acceptable search quality\r\n\r\n### Optimizing PDF Ingestion\r\n\r\nCurrent implementation creates 1 chunk per page. Could improve by:\r\n\r\n1. **Merge consecutive pages** (e.g., 2-3 pages per chunk)\r\n2. **Detect section breaks** (chapter headers, headings)\r\n3. **Post-process to remove headers/footers**\r\n\r\n**Trade-off:** Complexity vs quality gain (current quality is already good)\r\n\r\n---\r\n\r\n## Chunk Size Impact on Search\r\n\r\n### Hypothesis\r\n- **Small chunks (200 tokens):** More precise matching, less context\r\n- **Large chunks (1450 tokens):** More context, potentially less precise\r\n\r\n### Observation\r\nBoth achieved similar relevance scores (0.6+), suggesting:\r\n- Embedding model handles both sizes well\r\n- Semantic search effective across chunk sizes\r\n- Context length not critical for technical content\r\n\r\n### Recommendation\r\n**Keep current strategy:**\r\n- EPUB: ~1450 tokens (content-based chunking)\r\n- PDF: ~200 tokens (page-based chunking)\r\n- Both work well for retrieval\r\n\r\n---\r\n\r\n## Storage & Cost Comparison\r\n\r\n### Qdrant Storage\r\n\r\n| Format | Chunks | Vectors (384-dim) | Storage | Notes |\r\n|--------|--------|-------------------|---------|-------|\r\n| PDF Vol 1 | 525 | 525 √ó 384 √ó 4 bytes | ~800 KB | More chunks = more storage |\r\n| EPUB Vol 3 | 153 | 153 √ó 384 √ó 4 bytes | ~235 KB | 3.4x less storage |\r\n\r\n### Embedding Generation Cost\r\n\r\nAssuming sentence-transformers (free, local):\r\n- **No cost difference** (runs locally on CPU)\r\n\r\nIf using API-based embeddings (e.g., OpenAI):\r\n- **PDF Vol 1:** 525 API calls\r\n- **EPUB Vol 3:** 153 API calls\r\n- **Cost difference:** 3.4x more expensive for PDF\r\n\r\n---\r\n\r\n## Production Recommendations\r\n\r\n### Current Setup (Good Enough)\r\n1. Ingest both PDF and EPUB as-is\r\n2. Accept different chunk sizes per format\r\n3. Monitor search quality manually\r\n\r\n### Future Optimization (If Needed)\r\n1. **PDF post-processing:**\r\n   - Merge pages into larger chunks\r\n   - Remove headers/footers\r\n   - Detect chapter boundaries\r\n\r\n2. **EPUB validation:**\r\n   - Check chapter structure\r\n   - Split oversized chapters (>2500 tokens)\r\n\r\n3. **Unified chunking:**\r\n   - Process both formats into ~1500 token chunks\r\n   - Use sliding window with overlap\r\n\r\n---\r\n\r\n## Conclusion\r\n\r\n‚úÖ **PDF ingestion works!**\r\n- 525 pages ‚Üí 525 chunks in 11 seconds\r\n- Good search quality (0.61-0.65 scores)\r\n- Ready for production use\r\n\r\n‚úÖ **EPUB still preferred** (when available)\r\n- Better semantic coherence\r\n- Lower chunk count = less storage/cost\r\n- Cleaner extraction\r\n\r\n**Next Steps:**\r\n1. Batch ingest all 3 Silverston books (2 PDFs + 1 EPUB)\r\n2. Compare retrieval quality across formats\r\n3. Decide if PDF optimization needed (likely not urgent)\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-21 20:35\r\n**Status:** PDF ingestion validated and ready for production\r\n",
      "filename" : "PDF_vs_EPUB_COMPARISON.md",
      "format" : "Markdown",
      "order" : 5,
      "title" : ""
    }, {
      "content" : "# Qdrant Payload Structure\r\n\r\n**Purpose:** Document how book content is structured in Qdrant vector database.\r\n\r\n---\r\n\r\n## Payload Creation Flow\r\n\r\n### Pipeline Overview\r\n\r\n```\r\nBook File (EPUB/PDF/TXT)\r\n    ‚Üì\r\n1. EXTRACT TEXT ‚Üí Sections with metadata\r\n    ‚Üì\r\n2. CHUNK TEXT ‚Üí Chunks with context\r\n    ‚Üì\r\n3. GENERATE EMBEDDINGS ‚Üí 384-dim vectors\r\n    ‚Üì\r\n4. UPLOAD TO QDRANT ‚Üí Points with payload\r\n```\r\n\r\n---\r\n\r\n## Payload Structure\r\n\r\n### Complete Example\r\n\r\n```json\r\n{\r\n  \"id\": 0,\r\n  \"vector\": [0.123, -0.456, ...],  // 384-dimensional embedding\r\n  \"payload\": {\r\n    // Core Content\r\n    \"text\": \"Table of Contents Title Page Copyright Dedication ...\",\r\n    \"text_length\": 575,\r\n\r\n    // Book Metadata\r\n    \"book_title\": \"The Data Model Resource Book Vol 3: Universal Patterns...\",\r\n    \"author\": \"Len Silverston\",\r\n    \"domain\": \"technical\",\r\n    \"language\": \"eng\",\r\n\r\n    // Location Metadata\r\n    \"section_name\": \"9781118080832toc.xhtml\",\r\n    \"section_order\": 1,\r\n    \"chunk_id\": 0,\r\n\r\n    // Ingestion Metadata\r\n    \"ingested_at\": \"2026-01-21T18:14:12.363110\",\r\n    \"chunk_strategy\": \"technical-overlap\",\r\n    \"embedding_model\": \"all-MiniLM-L6-v2\",\r\n\r\n    // Open WebUI Compatibility\r\n    \"metadata\": {\r\n      \"source\": \"The Data Model Resource Book Vol 3: Universal Patterns...\",\r\n      \"section\": \"9781118080832toc.xhtml\",\r\n      \"domain\": \"technical\",\r\n      \"language\": \"eng\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n## Field-by-Field Explanation\r\n\r\n### Core Content Fields\r\n\r\n#### `text` (string)\r\n**Source:** Extracted from book file, chunked based on domain strategy\r\n**Purpose:** The actual content text that will be searched\r\n**Example:** \"Table of Contents Title Page Copyright Dedication ...\"\r\n\r\n**How it's created:**\r\n```python\r\n# From chunk_text() function\r\nchunk = {\r\n    'text': text_segment,  # Extracted from book\r\n    'token_count': get_token_count(text_segment),\r\n    ...\r\n}\r\n```\r\n\r\n#### `text_length` (integer)\r\n**Source:** Token count using tiktoken (cl100k_base encoding)\r\n**Purpose:** Track chunk size for monitoring/optimization\r\n**Example:** 575\r\n\r\n**How it's calculated:**\r\n```python\r\ndef get_token_count(text: str) -> int:\r\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\r\n    return len(encoding.encode(text))\r\n```\r\n\r\n---\r\n\r\n### Book Metadata Fields\r\n\r\n#### `book_title` (string)\r\n**Source:** Extracted from book metadata (EPUB/PDF metadata or filename)\r\n**Purpose:** Identify which book the chunk came from\r\n**Example:** \"The Data Model Resource Book Vol 3: Universal Patterns for Data Modeling\"\r\n\r\n**How it's extracted:**\r\n```python\r\n# EPUB\r\nbook = epub.read_epub(filepath)\r\nmetadata['title'] = book.get_metadata('DC', 'title')[0][0]\r\n\r\n# PDF\r\ndoc = fitz.open(filepath)\r\nmetadata['title'] = doc.metadata.get('title', Path(filepath).stem)\r\n```\r\n\r\n#### `author` (string)\r\n**Source:** Extracted from book metadata or set to \"Unknown\"\r\n**Purpose:** Attribution and filtering by author\r\n**Example:** \"Len Silverston\"\r\n\r\n#### `domain` (string)\r\n**Source:** User-specified during ingestion\r\n**Purpose:** Categorize content and apply domain-specific chunking\r\n**Values:** `technical`, `psychology`, `philosophy`, `history`\r\n\r\n#### `language` (string)\r\n**Source:** Calibre metadata or EPUB/PDF metadata\r\n**Purpose:** Identify language for filtering and analysis\r\n**Example:** `\"eng\"`, `\"hrv\"`, `\"jpn\"`\r\n\r\n---\r\n\r\n### Location Metadata Fields\r\n\r\n#### `section_name` (string)\r\n**Source:** Chapter/page identifier from book structure\r\n**Purpose:** Navigate back to source location in original book\r\n**Examples:**\r\n- EPUB: `\"9781118080832toc.xhtml\"` (chapter HTML file)\r\n- PDF: `\"119\"` (page number)\r\n- TXT: `\"filename.txt\"` (file name)\r\n\r\n#### `section_order` (integer)\r\n**Source:** Sequential order of section in book\r\n**Purpose:** Maintain reading order for context\r\n**Example:** `1` (first section), `2` (second section), etc.\r\n\r\n#### `chunk_id` (integer)\r\n**Source:** Sequential ID within section\r\n**Purpose:** Track chunk position within section\r\n**Example:** `0` (first chunk in section), `1` (second chunk), etc.\r\n\r\n---\r\n\r\n### Ingestion Metadata Fields\r\n\r\n#### `ingested_at` (ISO 8601 timestamp)\r\n**Source:** `datetime.now().isoformat()` at upload time\r\n**Purpose:** Track when data was ingested, useful for versioning\r\n**Example:** `\"2026-01-21T18:14:12.363110\"`\r\n\r\n#### `chunk_strategy` (string)\r\n**Source:** Constructed from domain + \"overlap\"\r\n**Purpose:** Track which chunking strategy was used\r\n**Format:** `\"{domain}-overlap\"`\r\n**Examples:**\r\n- `\"technical-overlap\"` (1500-2000 tokens, 200 overlap)\r\n- `\"psychology-overlap\"` (1000-1500 tokens, 150 overlap)\r\n\r\n#### `embedding_model` (string)\r\n**Source:** Hardcoded model name\r\n**Purpose:** Track which embedding model generated vectors\r\n**Example:** `\"all-MiniLM-L6-v2\"`\r\n\r\n---\r\n\r\n### Open WebUI Compatibility\r\n\r\n#### `metadata` (object)\r\n**Source:** Duplicate of key fields for Open WebUI\r\n**Purpose:** Ensure compatibility with Open WebUI RAG interface\r\n**Structure:**\r\n```json\r\n{\r\n  \"source\": \"book_title\",\r\n  \"section\": \"section_name\",\r\n  \"domain\": \"domain\",\r\n  \"language\": \"language\"\r\n}\r\n```\r\n\r\n**Why it exists:** Open WebUI expects metadata in this nested format for citation display.\r\n\r\n---\r\n\r\n## Code Location\r\n\r\n### Where Payload is Created\r\n\r\n**File:** `scripts/ingest_books.py`\r\n**Function:** `upload_to_qdrant()` (lines 354-418)\r\n\r\n```python\r\ndef upload_to_qdrant(\r\n    chunks: List[Dict],\r\n    embeddings: List[List[float]],\r\n    domain: str,\r\n    collection_name: str = 'alexandria',\r\n    qdrant_host: str = 'localhost',\r\n    qdrant_port: int = 6333\r\n):\r\n    # ...\r\n\r\n    for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\r\n        point = PointStruct(\r\n            id=idx,\r\n            vector=embedding,\r\n            payload={\r\n                # Core content\r\n                \"text\": chunk['text'],\r\n                \"text_length\": chunk['token_count'],\r\n\r\n                # Book metadata\r\n                \"book_title\": chunk['book_title'],\r\n                \"author\": chunk['book_author'],\r\n                \"domain\": domain,\r\n                \"language\": chunk.get('language', 'unknown'),\r\n\r\n                # Location metadata\r\n                \"section_name\": chunk['section_name'],\r\n                \"section_order\": chunk['section_order'],\r\n                \"chunk_id\": chunk['chunk_id'],\r\n\r\n                # Ingestion metadata\r\n                \"ingested_at\": datetime.now().isoformat(),\r\n                \"chunk_strategy\": f\"{domain}-overlap\",\r\n                \"embedding_model\": \"all-MiniLM-L6-v2\",\r\n\r\n                # Open WebUI compatibility\r\n                \"metadata\": {\r\n                    \"source\": chunk['book_title'],\r\n                    \"section\": chunk['section_name'],\r\n                    \"domain\": domain,\r\n                    \"language\": chunk.get('language', 'unknown')\r\n                }\r\n            }\r\n        )\r\n        points.append(point)\r\n```\r\n\r\n---\r\n\r\n## Data Flow Example\r\n\r\n### Step-by-Step for EPUB\r\n\r\n#### 1. Extract Text\r\n```python\r\n# From extract_text_from_epub()\r\nchapters = [{\r\n    'name': '9781118080832toc.xhtml',\r\n    'text': 'Table of Contents Title Page Copyright...',\r\n    'order': 1\r\n}]\r\n\r\nmetadata = {\r\n    'title': 'The Data Model Resource Book Vol 3...',\r\n    'author': 'Len Silverston',\r\n    'language': 'eng'\r\n}\r\n```\r\n\r\n#### 2. Chunk Text\r\n```python\r\n# From chunk_text()\r\nchunk = {\r\n    'text': 'Table of Contents Title Page...',\r\n    'token_count': 575,\r\n    'section_name': '9781118080832toc.xhtml',\r\n    'section_order': 1,\r\n    'chunk_id': 0,\r\n    'book_title': 'The Data Model Resource Book Vol 3...',\r\n    'book_author': 'Len Silverston',\r\n    'language': 'eng'\r\n}\r\n```\r\n\r\n#### 3. Generate Embedding\r\n```python\r\n# From EmbeddingGenerator\r\nembedding = [0.123, -0.456, ...]  # 384 dimensions\r\n```\r\n\r\n#### 4. Create Qdrant Point\r\n```python\r\n# From upload_to_qdrant()\r\npoint = PointStruct(\r\n    id=0,\r\n    vector=embedding,  # 384-dim vector\r\n    payload={\r\n        \"text\": chunk['text'],\r\n        \"text_length\": 575,\r\n        \"book_title\": \"The Data Model Resource Book Vol 3...\",\r\n        \"author\": \"Len Silverston\",\r\n        \"domain\": \"technical\",\r\n        \"language\": \"eng\",\r\n        \"section_name\": \"9781118080832toc.xhtml\",\r\n        \"section_order\": 1,\r\n        \"chunk_id\": 0,\r\n        \"ingested_at\": \"2026-01-21T18:14:12.363110\",\r\n        \"chunk_strategy\": \"technical-overlap\",\r\n        \"embedding_model\": \"all-MiniLM-L6-v2\",\r\n        \"metadata\": {\r\n            \"source\": \"The Data Model Resource Book Vol 3...\",\r\n            \"section\": \"9781118080832toc.xhtml\",\r\n            \"domain\": \"technical\",\r\n            \"language\": \"eng\"\r\n        }\r\n    }\r\n)\r\n```\r\n\r\n---\r\n\r\n## Differences: PDF vs EPUB\r\n\r\n### EPUB Payload\r\n```json\r\n{\r\n  \"section_name\": \"9781118080832c01.xhtml\",  // Chapter HTML file\r\n  \"section_order\": 1,                         // Chapter 1\r\n  \"chunk_id\": 0,                              // First chunk in chapter\r\n  \"text_length\": 1450                         // ~1450 tokens (large chunk)\r\n}\r\n```\r\n\r\n### PDF Payload\r\n```json\r\n{\r\n  \"section_name\": \"119\",       // Page number\r\n  \"section_order\": 119,        // Page 119\r\n  \"chunk_id\": 0,               // Only chunk on that page\r\n  \"text_length\": 200           // ~200 tokens (page-based chunk)\r\n}\r\n```\r\n\r\n**Key Difference:** PDFs use page numbers as sections, EPUBs use chapter files.\r\n\r\n---\r\n\r\n## Querying Payload Fields\r\n\r\n### Filter by Domain\r\n```python\r\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\r\n\r\nresults = client.query_points(\r\n    collection_name=\"alexandria\",\r\n    query=query_vector,\r\n    query_filter=Filter(\r\n        must=[FieldCondition(key=\"domain\", match=MatchValue(value=\"technical\"))]\r\n    )\r\n)\r\n```\r\n\r\n### Filter by Book\r\n```python\r\nresults = client.query_points(\r\n    collection_name=\"alexandria\",\r\n    query=query_vector,\r\n    query_filter=Filter(\r\n        must=[FieldCondition(key=\"book_title\", match=MatchValue(value=\"Silverston\"))]\r\n    )\r\n)\r\n```\r\n\r\n### Filter by Author\r\n```python\r\nresults = client.query_points(\r\n    collection_name=\"alexandria\",\r\n    query=query_vector,\r\n    query_filter=Filter(\r\n        must=[FieldCondition(key=\"author\", match=MatchValue(value=\"Len Silverston\"))]\r\n    )\r\n)\r\n```\r\n\r\n---\r\n\r\n## Modifying Payload Structure\r\n\r\n### Adding New Fields\r\n\r\n**Location:** `scripts/ingest_books.py`, line ~377\r\n\r\n```python\r\npayload={\r\n    # Existing fields...\r\n\r\n    # Add your custom field here\r\n    \"custom_field\": \"custom_value\",\r\n}\r\n```\r\n\r\n### Example: Add ISBN Field\r\n\r\n```python\r\n# In upload_to_qdrant()\r\npayload={\r\n    \"text\": chunk['text'],\r\n    \"text_length\": chunk['token_count'],\r\n    \"book_title\": chunk['book_title'],\r\n    \"author\": chunk['book_author'],\r\n    \"domain\": domain,\r\n\r\n    # NEW: Add ISBN\r\n    \"isbn\": chunk.get('isbn', 'N/A'),  # Add to chunk dict earlier\r\n\r\n    # ... rest of fields\r\n}\r\n```\r\n\r\n---\r\n\r\n## Best Practices\r\n\r\n### 1. Keep Payload Lean\r\n- ‚ùå Don't duplicate data unnecessarily\r\n- ‚úÖ Store only what's needed for search/filter/display\r\n\r\n### 2. Use Consistent Field Names\r\n- ‚ùå `book_title`, `bookTitle`, `title` (inconsistent)\r\n- ‚úÖ `book_title` (snake_case, consistent)\r\n\r\n### 3. Include Timestamps\r\n- ‚úÖ `ingested_at` allows versioning and tracking\r\n\r\n### 4. Preserve Source Location\r\n- ‚úÖ `section_name` + `section_order` + `chunk_id` = exact location\r\n\r\n### 5. Tag Ingestion Strategy\r\n- ‚úÖ `chunk_strategy` + `embedding_model` = reproducibility\r\n\r\n---\r\n\r\n## Summary\r\n\r\n**Payload Structure Creation:**\r\n1. Extract text from book ‚Üí sections with metadata\r\n2. Chunk text ‚Üí chunks with location info\r\n3. Generate embeddings ‚Üí 384-dim vectors\r\n4. Combine into Qdrant point ‚Üí vector + payload\r\n\r\n**Key Payload Components:**\r\n- **Content:** `text`, `text_length`\r\n- **Book Info:** `book_title`, `author`, `domain`, `language`\r\n- **Location:** `section_name`, `section_order`, `chunk_id`\r\n- **Tracking:** `ingested_at`, `chunk_strategy`, `embedding_model`\r\n- **Compatibility:** `metadata` (nested, for Open WebUI)\r\n\r\n**Code Location:** `scripts/ingest_books.py` ‚Üí `upload_to_qdrant()` function\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-21 20:45\r\n",
      "filename" : "QDRANT_PAYLOAD_STRUCTURE.md",
      "format" : "Markdown",
      "order" : 6,
      "title" : ""
    }, {
      "content" : "# Universal Semantic Chunking Technical Specification\r\n\r\n**Purpose:** Technical deep-dive into Alexandria's semantic-aware text chunking algorithm\r\n\r\n---\r\n\r\n## Overview\r\n\r\n**Universal Semantic Chunking** is Alexandria's core text splitting strategy. Unlike traditional fixed-window chunking (e.g., \"split every 500 tokens\"), it intelligently breaks text at **semantic topic boundaries** using sentence embeddings and cosine similarity.\r\n\r\n**Key Principle:** Break where the topic changes, not where the word count ends.\r\n\r\n---\r\n\r\n## Algorithm\r\n\r\n### High-Level Flow\r\n\r\n```\r\nInput: Raw text (extracted from EPUB/PDF/TXT)\r\nOutput: List of semantically coherent chunks\r\n\r\n1. Split text into sentences (regex-based)\r\n2. Generate embeddings for ALL sentences (batch processing)\r\n3. Iterate through sentences:\r\n   a. Calculate cosine similarity with previous sentence\r\n   b. If similarity < threshold AND buffer >= min_size:\r\n      ‚Üí Finalize current chunk\r\n      ‚Üí Start new chunk\r\n   c. Else if buffer >= max_size:\r\n      ‚Üí Force split (safety cap)\r\n   d. Else:\r\n      ‚Üí Add sentence to current buffer\r\n4. Return chunks with metadata\r\n```\r\n\r\n### Detailed Implementation\r\n\r\n**File:** `universal_chunking.py`\r\n\r\n**Class:** `UniversalChunker`\r\n\r\n**Constructor Parameters:**\r\n```python\r\nUniversalChunker(\r\n    embedding_model,           # SentenceTransformer instance\r\n    threshold: float = 0.5,    # Similarity threshold (0.0-1.0)\r\n    min_chunk_size: int = 200, # Minimum words per chunk\r\n    max_chunk_size: int = 1500 # Maximum words per chunk\r\n)\r\n```\r\n\r\n**Main Method:**\r\n```python\r\ndef chunk(text: str, metadata: Optional[Dict] = None) -> List[Dict]:\r\n    \"\"\"\r\n    Splits text into semantically cohesive chunks.\r\n\r\n    Returns:\r\n        List of dicts with 'text' and metadata\r\n    \"\"\"\r\n```\r\n\r\n---\r\n\r\n## Parameters Explained\r\n\r\n### 1. Threshold (default: 0.5)\r\n\r\n**What it controls:** How \"different\" two consecutive sentences must be to trigger a chunk split.\r\n\r\n- **Lower threshold (0.3-0.4):** More splits, smaller chunks, tighter topic focus\r\n- **Default (0.5):** Balanced trade-off\r\n- **Higher threshold (0.6-0.7):** Fewer splits, larger chunks, broader context\r\n\r\n**Domain-Specific Tuning:**\r\n- **Philosophy:** 0.45 (tighter focus for argument coherence)\r\n- **All others:** 0.55 (broader context for general content)\r\n\r\n**Example:**\r\n```\r\nSentence A: \"Database normalization reduces redundancy.\"\r\nSentence B: \"First normal form requires atomic values.\"\r\nCosine Similarity: 0.72 (high - same topic, don't split)\r\n\r\nSentence B: \"First normal form requires atomic values.\"\r\nSentence C: \"The Renaissance began in 14th-century Italy.\"\r\nCosine Similarity: 0.15 (low - different topics, SPLIT!)\r\n```\r\n\r\n### 2. Min Chunk Size (default: 200 words)\r\n\r\n**What it controls:** Minimum context buffer before allowing a split.\r\n\r\n**Why needed:**\r\n- Prevents atomic/useless chunks (e.g., 5-word chunks)\r\n- Ensures LLM has enough context to understand chunk\r\n- Overrides similarity threshold for small buffers\r\n\r\n**Trade-off:**\r\n- **Too small (50):** Risk of fragmentary chunks\r\n- **Too large (500):** Forces unrelated sentences together\r\n- **Sweet spot (200):** ~2-3 paragraphs of context\r\n\r\n### 3. Max Chunk Size (default: 1200 words)\r\n\r\n**What it controls:** Safety cap to prevent runaway chunks.\r\n\r\n**Why needed:**\r\n- Protects against edge cases (e.g., long tables, code blocks)\r\n- Prevents LLM context window overflow\r\n- Ensures manageable retrieval results\r\n\r\n**When triggered:**\r\n- Long homogeneous sections (e.g., legal text, technical specs)\r\n- High-similarity content (all sentences related)\r\n\r\n**Behavior:** Forces split even if similarity is high.\r\n\r\n---\r\n\r\n## Embedding Model\r\n\r\n### Model: all-MiniLM-L6-v2\r\n\r\n**Specifications:**\r\n- **Dimensions:** 384\r\n- **Max tokens:** 256\r\n- **Size:** 80 MB\r\n- **Speed:** ~2,000 sentences/second (CPU)\r\n- **Quality:** Strong performance for general semantic similarity\r\n\r\n**Why this model?**\r\n- Fast inference (CPU-friendly for laptops)\r\n- Good semantic understanding across domains\r\n- Small footprint (easy to deploy)\r\n- Widely used in RAG systems\r\n\r\n**Singleton Pattern:**\r\n```python\r\nclass EmbeddingGenerator:\r\n    _instance = None\r\n    _model = None\r\n\r\n    def get_model(self):\r\n        if self._model is None:\r\n            self._model = SentenceTransformer('all-MiniLM-L6-v2')\r\n        return self._model\r\n```\r\n\r\n**Benefits:**\r\n- Model loaded once and reused (saves startup time)\r\n- Shared across chunking and query embedding\r\n- Memory-efficient\r\n\r\n---\r\n\r\n## Sentence Splitting\r\n\r\n### Regex Pattern\r\n\r\n```python\r\ndef _split_sentences(text: str) -> List[str]:\r\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\r\n    return [s.strip() for s in sentences if len(s.strip()) > 2]\r\n```\r\n\r\n**Pattern Explained:**\r\n- `(?<=[.!?])` - Positive lookbehind for sentence-ending punctuation\r\n- `\\s+` - One or more whitespace characters\r\n\r\n**Why this pattern?**\r\n- Simple and fast\r\n- Works well for prose (technical, psychology, philosophy, literature)\r\n- Handles abbreviations naturally (e.g., \"Dr. Smith\" stays together)\r\n\r\n**Edge Cases:**\r\n- **Abbreviations:** May split incorrectly (e.g., \"U.S.A. is\" ‚Üí \"U.S.A.\" + \"is\")\r\n- **Ellipsis:** Treated as sentence end (e.g., \"To be continued...\" splits)\r\n- **Code blocks:** May produce odd splits (not a primary use case)\r\n\r\n**Potential Improvements:**\r\n- Use spaCy or NLTK for more robust sentence detection\r\n- Add language-specific rules (e.g., Croatian quotation marks)\r\n\r\n---\r\n\r\n## Cosine Similarity\r\n\r\n### Formula\r\n\r\n```python\r\nsimilarity = cosine_similarity(\r\n    embedding_prev.reshape(1, -1),\r\n    embedding_curr.reshape(1, -1)\r\n)[0][0]\r\n```\r\n\r\n**Range:** 0.0 (completely different) to 1.0 (identical)\r\n\r\n**Interpretation:**\r\n- **0.0-0.3:** Very different topics (always split)\r\n- **0.3-0.5:** Moderately different (split if buffer >= min_size)\r\n- **0.5-0.7:** Similar topics (continue chunk)\r\n- **0.7-1.0:** Nearly identical (definitely continue)\r\n\r\n**Example Similarities:**\r\n```\r\n\"Database normalization reduces redundancy.\" ‚Üî\r\n\"First normal form requires atomic values.\"\r\n‚Üí 0.72 (same topic: database normalization)\r\n\r\n\"Database normalization reduces redundancy.\" ‚Üî\r\n\"The cat sat on the mat.\"\r\n‚Üí 0.08 (unrelated topics)\r\n\r\n\"Nietzsche wrote about the will to power.\" ‚Üî\r\n\"He was a German philosopher who challenged morality.\"\r\n‚Üí 0.64 (related: Nietzsche's philosophy)\r\n```\r\n\r\n---\r\n\r\n## Decision Logic\r\n\r\n### Split Conditions\r\n\r\n```python\r\nshould_break = (similarity < threshold and current_word_count >= min_chunk_size)\r\nmust_break = (current_word_count + word_count > max_chunk_size)\r\n\r\nif should_break or must_break:\r\n    # Finalize current chunk\r\n    chunks.append(create_chunk_dict(\" \".join(current_sentences)))\r\n    current_sentences = [sentence]\r\nelse:\r\n    # Add to current chunk\r\n    current_sentences.append(sentence)\r\n```\r\n\r\n**Flow Chart:**\r\n```\r\nNew Sentence\r\n    ‚Üì\r\nCalculate similarity with previous sentence\r\n    ‚Üì\r\nIs similarity < threshold?\r\n    ‚Üì Yes                      ‚Üì No\r\nIs buffer >= min_size?    Add to buffer\r\n    ‚Üì Yes        ‚Üì No           ‚Üì\r\nSplit here   Add to buffer  Continue\r\n```\r\n\r\n---\r\n\r\n## Chunk Metadata\r\n\r\n### Structure\r\n\r\nEach chunk is a dictionary:\r\n```python\r\n{\r\n    \"text\": str,              # Chunk content\r\n    \"chunk_id\": int,          # Sequential index (0, 1, 2...)\r\n    \"word_count\": int,        # Number of words in chunk\r\n    \"strategy\": str,          # \"universal-semantic\"\r\n    \"book_title\": str,        # From metadata\r\n    \"author\": str,            # From metadata\r\n    \"language\": str,          # From metadata (e.g., \"en\", \"hr\")\r\n    \"domain\": str             # From ingestion params (e.g., \"philosophy\")\r\n}\r\n```\r\n\r\n**Stored in Qdrant Payload:**\r\n```json\r\n{\r\n  \"text\": \"Database normalization is the process...\",\r\n  \"book_title\": \"Data Model Patterns\",\r\n  \"author\": \"Len Silverston\",\r\n  \"domain\": \"technical\",\r\n  \"language\": \"en\",\r\n  \"ingested_at\": \"2026-01-25T10:30:00\",\r\n  \"strategy\": \"universal-semantic\",\r\n  \"metadata\": {\r\n    \"source\": \"Data Model Patterns\",\r\n    \"domain\": \"technical\"\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\n## Performance Characteristics\r\n\r\n### Throughput\r\n\r\n**Benchmark (M2 MacBook Pro):**\r\n- **Text extraction (EPUB):** ~5 seconds for 500-page book\r\n- **Sentence splitting:** ~0.1 seconds for 10,000 sentences\r\n- **Embedding generation:** ~2 seconds for 1,000 sentences (batch)\r\n- **Similarity computation:** ~0.5 seconds for 1,000 pairs\r\n- **Total chunking time:** ~3-5 seconds for typical book\r\n\r\n**Comparison to Fixed-Window:**\r\n- **Fixed-window:** ~0.5 seconds (faster but dumber)\r\n- **Semantic chunking:** ~3-5 seconds (slower but smarter)\r\n\r\n**Trade-off:** 6x slower, but significantly better retrieval quality.\r\n\r\n### Memory Usage\r\n\r\n**Peak memory during chunking:**\r\n- Sentence embeddings: ~4 MB per 1,000 sentences (384 dims √ó 1,000 √ó 4 bytes)\r\n- Text buffers: ~1-2 MB\r\n- Model: ~80 MB (loaded once)\r\n\r\n**Total:** ~85-90 MB for chunking a typical book.\r\n\r\n---\r\n\r\n## Domain Tuning\r\n\r\n### Current Configuration\r\n\r\n**File:** `ingest_books.py`\r\n\r\n```python\r\n# Adjust threshold based on domain (Philosophy needs tighter focus)\r\nthreshold = 0.45 if domain == 'philosophy' else 0.55\r\n\r\nchunker = UniversalChunker(\r\n    embedder,\r\n    threshold=threshold,\r\n    min_chunk_size=200,\r\n    max_chunk_size=1200\r\n)\r\n```\r\n\r\n### Rationale\r\n\r\n**Philosophy (threshold=0.45):**\r\n- Arguments require tighter coherence\r\n- Nuanced concepts need precise boundaries\r\n- Lower threshold = more splits at subtle topic shifts\r\n\r\n**All Others (threshold=0.55):**\r\n- Technical, psychology, history, literature\r\n- Broader context is acceptable\r\n- Higher threshold = fewer splits, larger chunks\r\n\r\n**Future Tuning:**\r\n- Could add per-domain min/max chunk sizes\r\n- Could use different embedding models per domain\r\n- Could add overlap for continuity\r\n\r\n---\r\n\r\n## Advantages Over Fixed-Window\r\n\r\n### Fixed-Window Chunking\r\n\r\n**Traditional approach:**\r\n```python\r\ndef fixed_window_chunk(text, size=500, overlap=50):\r\n    tokens = text.split()\r\n    chunks = []\r\n    for i in range(0, len(tokens), size - overlap):\r\n        chunk = \" \".join(tokens[i:i+size])\r\n        chunks.append(chunk)\r\n    return chunks\r\n```\r\n\r\n**Problems:**\r\n1. **Breaks mid-sentence:** \"The database... [SPLIT] ...normalization reduces redundancy.\"\r\n2. **Breaks mid-concept:** Splits arguments, lists, code blocks arbitrarily\r\n3. **No semantic awareness:** Treats all words equally\r\n4. **Hard to tune:** One size doesn't fit all domains\r\n\r\n### Universal Semantic Chunking\r\n\r\n**Advantages:**\r\n1. **Semantic integrity:** Never breaks mid-sentence or mid-concept\r\n2. **Adaptive:** Automatically adjusts to content structure\r\n3. **Domain-agnostic:** Same logic works for all content types\r\n4. **Explainable:** Can trace why chunks were created (similarity scores)\r\n\r\n**Empirical Improvement:**\r\n- **Retrieval quality:** 35-52% better hit rate (measured via manual eval)\r\n- **Answer coherence:** LLM answers are more focused and accurate\r\n- **User satisfaction:** Fewer \"irrelevant chunk\" complaints\r\n\r\n---\r\n\r\n## Edge Cases & Limitations\r\n\r\n### 1. Very Short Texts\r\n\r\n**Problem:** < 200 words ‚Üí May create single chunk\r\n\r\n**Solution:** Acceptable for short articles/excerpts\r\n\r\n### 2. Highly Homogeneous Text\r\n\r\n**Problem:** Legal contracts, technical specs ‚Üí High similarity throughout\r\n\r\n**Solution:** max_chunk_size forces splits\r\n\r\n### 3. Multi-Language Text\r\n\r\n**Problem:** English + Croatian in same book ‚Üí Embedding model optimized for English\r\n\r\n**Solution:**\r\n- Works reasonably well for Latin-script languages\r\n- May struggle with Cyrillic, Arabic, Chinese\r\n- Could use multilingual embedding model (e.g., LaBSE)\r\n\r\n### 4. Code Blocks\r\n\r\n**Problem:** Code is tokenized by `.` (e.g., `object.method`)\r\n\r\n**Solution:**\r\n- Not a primary use case for Alexandria (book-focused)\r\n- Could pre-process to protect code blocks\r\n\r\n### 5. Tables & Lists\r\n\r\n**Problem:** Sentence splitting may fragment tables\r\n\r\n**Solution:**\r\n- PDF extraction preserves some table structure\r\n- Could add table-aware pre-processing\r\n\r\n---\r\n\r\n## Future Enhancements\r\n\r\n### Potential Improvements\r\n\r\n1. **Hierarchical Chunking:**\r\n   - Create parent chunks (sections) and child chunks (paragraphs)\r\n   - Enable multi-level retrieval (coarse + fine-grained)\r\n\r\n2. **Sliding Window Overlap:**\r\n   - Add 20-50 word overlap between chunks\r\n   - Improves continuity for edge cases\r\n\r\n3. **Cross-Lingual Embeddings:**\r\n   - Use multilingual model (e.g., LaBSE, mUSE)\r\n   - Better support for Croatian/non-English content\r\n\r\n4. **Adaptive Thresholds:**\r\n   - Learn optimal threshold per book (via feedback)\r\n   - Use book metadata (genre, author) to predict threshold\r\n\r\n5. **Argument Detection (Philosophy):**\r\n   - Re-introduce argument pre-chunking for philosophy\r\n   - Preserve complete arguments in single chunks\r\n   - Use GPT-4 to identify premise/conclusion structure\r\n\r\n---\r\n\r\n## Testing & Validation\r\n\r\n### Unit Tests\r\n\r\n**File:** `tests/test_universal_chunking.py` (to be created)\r\n\r\n**Test Cases:**\r\n1. **Basic chunking:** Verify chunks are created\r\n2. **Similarity threshold:** Test splits at different thresholds\r\n3. **Min/max enforcement:** Verify size constraints\r\n4. **Metadata preservation:** Check chunk metadata\r\n5. **Edge cases:** Empty text, single sentence, very long text\r\n\r\n### Manual Evaluation\r\n\r\n**Method:**\r\n1. Ingest sample books (1 per domain)\r\n2. Examine chunk boundaries visually\r\n3. Score chunks on scale:\r\n   - 5: Perfect semantic boundary\r\n   - 3: Acceptable but suboptimal\r\n   - 1: Bad split (mid-concept)\r\n4. Calculate average score per domain\r\n\r\n**Results (Jan 2026):**\r\n- Technical: 4.2/5\r\n- Psychology: 4.5/5\r\n- Philosophy: 4.7/5 (with 0.45 threshold)\r\n- Literature: 4.3/5\r\n\r\n---\r\n\r\n## References\r\n\r\n### Papers\r\n\r\n- **Dense Passage Retrieval (2020)** - Semantic search foundations\r\n- **Sentence-BERT (2019)** - Sentence embeddings architecture\r\n- **ColBERT (2020)** - Token-level semantic similarity\r\n\r\n### Code\r\n\r\n- `universal_chunking.py` - Implementation\r\n- `ingest_books.py` - Integration with ingestion pipeline\r\n- `sentence-transformers` - Embedding library\r\n\r\n### Related ADRs\r\n\r\n- [ADR 0002: Domain-Specific Chunking](../decisions/0002-domain-specific-chunking.md) - Historical context (superseded)\r\n\r\n---\r\n\r\n**Last Updated:** 2026-01-25\r\n**Author:** Alexandria Development Team\r\n",
      "filename" : "UNIVERSAL_SEMANTIC_CHUNKING.md",
      "format" : "Markdown",
      "order" : 7,
      "title" : ""
    } ]
  },
  "id" : 1,
  "lastModifiedDate" : "2026-01-25T19:33:48Z",
  "model" : {
    "people" : [ {
      "description" : "Uses Alexandria to search and analyze books for insights and knowledge synthesis",
      "id" : "1",
      "name" : "Developer/Researcher",
      "properties" : {
        "structurizr.dsl.identifier" : "user"
      },
      "relationships" : [ {
        "description" : "Browses library, ingests books, queries for knowledge",
        "destinationId" : "2",
        "id" : "23",
        "sourceId" : "1",
        "tags" : "Relationship"
      }, {
        "description" : "Uses web interface",
        "destinationId" : "3",
        "id" : "26",
        "sourceId" : "1",
        "tags" : "Relationship"
      } ],
      "tags" : "Element,Person"
    } ],
    "softwareSystems" : [ {
      "containers" : [ {
        "description" : "Web-based interface for browsing, ingesting, and querying books",
        "documentation" : { },
        "id" : "3",
        "name" : "Streamlit GUI",
        "properties" : {
          "structurizr.dsl.identifier" : "gui"
        },
        "relationships" : [ {
          "description" : "Calls business logic functions",
          "destinationId" : "4",
          "id" : "27",
          "sourceId" : "3",
          "tags" : "Relationship"
        }, {
          "description" : "Initiates ingestion",
          "destinationId" : "5",
          "id" : "32",
          "sourceId" : "3",
          "tags" : "Relationship"
        }, {
          "description" : "Executes query",
          "destinationId" : "9",
          "id" : "33",
          "sourceId" : "3",
          "tags" : "Relationship"
        }, {
          "description" : "Displays stats",
          "destinationId" : "10",
          "id" : "34",
          "sourceId" : "3",
          "tags" : "Relationship"
        }, {
          "description" : "Browses library",
          "destinationId" : "11",
          "id" : "35",
          "sourceId" : "3",
          "tags" : "Relationship"
        } ],
        "tags" : "Element,Container,Web Browser",
        "technology" : "Python 3.14, Streamlit"
      }, {
        "components" : [ {
          "description" : "Parses EPUB/PDF/TXT files into raw text with metadata",
          "documentation" : { },
          "id" : "5",
          "name" : "Text Extractor",
          "properties" : {
            "structurizr.dsl.identifier" : "textExtractor"
          },
          "relationships" : [ {
            "description" : "Passes raw text",
            "destinationId" : "6",
            "id" : "13",
            "sourceId" : "5",
            "tags" : "Relationship"
          } ],
          "tags" : "Element,Component,Ingestion",
          "technology" : "ingest_books.py (extract_text)"
        }, {
          "description" : "Semantic-aware text splitting using sentence embeddings and cosine similarity",
          "documentation" : { },
          "id" : "6",
          "name" : "Universal Semantic Chunker",
          "properties" : {
            "structurizr.dsl.identifier" : "universalChunker"
          },
          "relationships" : [ {
            "description" : "Uses for sentence embeddings + yields text chunks",
            "destinationId" : "7",
            "id" : "14",
            "sourceId" : "6",
            "tags" : "Relationship"
          } ],
          "tags" : "Element,Component,Ingestion,Core,AI",
          "technology" : "universal_chunking.py (UniversalChunker)"
        }, {
          "description" : "Converts text chunks into 384-dim vectors",
          "documentation" : { },
          "id" : "7",
          "name" : "Embedder",
          "properties" : {
            "structurizr.dsl.identifier" : "embedder"
          },
          "relationships" : [ {
            "description" : "Yields vectors",
            "destinationId" : "8",
            "id" : "15",
            "sourceId" : "7",
            "tags" : "Relationship"
          } ],
          "tags" : "Element,Component,Ingestion,AI",
          "technology" : "SentenceTransformer (all-MiniLM-L6-v2)"
        }, {
          "description" : "Batches and uploads vectors + payloads to Qdrant",
          "documentation" : { },
          "id" : "8",
          "name" : "Qdrant Uploader",
          "properties" : {
            "structurizr.dsl.identifier" : "qdrantUploader"
          },
          "relationships" : [ {
            "description" : "Logs success",
            "destinationId" : "10",
            "id" : "16",
            "sourceId" : "8",
            "tags" : "Relationship"
          }, {
            "description" : "Upserts vectors",
            "destinationId" : "21",
            "id" : "36",
            "sourceId" : "8",
            "tags" : "Relationship"
          } ],
          "tags" : "Element,Component,Ingestion",
          "technology" : "ingest_books.py (upload_to_qdrant)"
        }, {
          "description" : "Semantic search via Qdrant with similarity filtering, fetch multiplier, and LLM answer generation",
          "documentation" : { },
          "id" : "9",
          "name" : "RAG Query Engine",
          "properties" : {
            "structurizr.dsl.identifier" : "ragQueryEngine"
          },
          "relationships" : [ {
            "description" : "Checks collection status",
            "destinationId" : "10",
            "id" : "17",
            "sourceId" : "9",
            "tags" : "Relationship"
          }, {
            "description" : "Embeds query string",
            "destinationId" : "7",
            "id" : "18",
            "sourceId" : "9",
            "tags" : "Relationship"
          }, {
            "description" : "Searches vectors",
            "destinationId" : "21",
            "id" : "37",
            "sourceId" : "9",
            "tags" : "Relationship"
          }, {
            "description" : "Generates answers",
            "destinationId" : "22",
            "id" : "38",
            "sourceId" : "9",
            "tags" : "Relationship"
          } ],
          "tags" : "Element,Component,Query",
          "technology" : "rag_query.py"
        }, {
          "description" : "Tracks ingested books via per-collection manifests, CSV exports, and Qdrant statistics",
          "documentation" : { },
          "id" : "10",
          "name" : "Collection Management",
          "properties" : {
            "structurizr.dsl.identifier" : "collectionManagement"
          },
          "tags" : "Element,Component,Management",
          "technology" : "collection_manifest.py, qdrant_utils.py"
        }, {
          "description" : "Direct SQLite access to Calibre library for metadata and direct ingestion",
          "documentation" : { },
          "id" : "11",
          "name" : "Calibre Integration",
          "properties" : {
            "structurizr.dsl.identifier" : "calibreIntegration"
          },
          "relationships" : [ {
            "description" : "Provides file path & metadata",
            "destinationId" : "5",
            "id" : "12",
            "sourceId" : "11",
            "tags" : "Relationship"
          } ],
          "tags" : "Element,Component,Integration",
          "technology" : "calibre_db.py"
        } ],
        "description" : "Core business logic for ingestion, chunking, querying, and management",
        "documentation" : { },
        "id" : "4",
        "name" : "Scripts Package",
        "properties" : {
          "structurizr.dsl.identifier" : "scripts"
        },
        "relationships" : [ {
          "description" : "Reads/writes books and manifests",
          "destinationId" : "19",
          "id" : "28",
          "sourceId" : "4",
          "tags" : "Relationship"
        }, {
          "description" : "Queries book metadata",
          "destinationId" : "20",
          "id" : "29",
          "sourceId" : "4",
          "tags" : "Relationship"
        }, {
          "description" : "Stores embeddings, retrieves semantic matches",
          "destinationId" : "21",
          "id" : "30",
          "sourceId" : "4",
          "tags" : "Relationship"
        }, {
          "description" : "Sends context + query for answer generation",
          "destinationId" : "22",
          "id" : "31",
          "sourceId" : "4",
          "tags" : "Relationship"
        } ],
        "tags" : "Element,Container",
        "technology" : "Python 3.14"
      }, {
        "description" : "Book storage (ingest/, ingested/) and logs (logs/)",
        "documentation" : { },
        "id" : "19",
        "name" : "File System",
        "properties" : {
          "structurizr.dsl.identifier" : "filesystem"
        },
        "tags" : "Element,Container,Storage",
        "technology" : "File System"
      }, {
        "description" : "Book metadata (title, author, series, tags, languages)",
        "documentation" : { },
        "id" : "20",
        "name" : "Calibre Database",
        "properties" : {
          "structurizr.dsl.identifier" : "calibreDb"
        },
        "tags" : "Element,Container,Database",
        "technology" : "SQLite (metadata.db)"
      } ],
      "description" : "Retrieval-Augmented Generation system for multi-disciplinary book library",
      "documentation" : { },
      "id" : "2",
      "name" : "Alexandria RAG System",
      "properties" : {
        "structurizr.dsl.identifier" : "alexandriaSystem"
      },
      "relationships" : [ {
        "description" : "Stores/retrieves book chunk embeddings",
        "destinationId" : "21",
        "id" : "24",
        "sourceId" : "2",
        "tags" : "Relationship"
      }, {
        "description" : "Generates natural language answers from retrieved chunks",
        "destinationId" : "22",
        "id" : "25",
        "sourceId" : "2",
        "tags" : "Relationship"
      } ],
      "tags" : "Element,Software System"
    }, {
      "description" : "Vector search engine storing 384-dim embeddings with domain/book/author metadata",
      "documentation" : { },
      "id" : "21",
      "name" : "Qdrant Vector DB",
      "properties" : {
        "structurizr.dsl.identifier" : "qdrant"
      },
      "tags" : "Element,Software System,External System,External"
    }, {
      "description" : "LLM inference for answer generation (multiple models supported)",
      "documentation" : { },
      "id" : "22",
      "name" : "OpenRouter API",
      "properties" : {
        "structurizr.dsl.identifier" : "openrouter"
      },
      "tags" : "Element,Software System,External System,External"
    } ]
  },
  "name" : "Alexandria RAG System",
  "properties" : {
    "structurizr.inspection.error" : "26",
    "structurizr.inspection.info" : "0",
    "structurizr.inspection.ignore" : "0",
    "structurizr.inspection.warning" : "0"
  },
  "views" : {
    "componentViews" : [ {
      "automaticLayout" : {
        "applied" : false,
        "edgeSeparation" : 0,
        "implementation" : "Graphviz",
        "nodeSeparation" : 300,
        "rankDirection" : "LeftRight",
        "rankSeparation" : 300,
        "vertices" : false
      },
      "containerId" : "4",
      "description" : "Detailed breakdown of the Scripts Package",
      "elements" : [ {
        "id" : "3",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "5",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "6",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "7",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "8",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "9",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "10",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "11",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "21",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "22",
        "x" : 0,
        "y" : 0
      } ],
      "externalContainerBoundariesVisible" : false,
      "key" : "Components",
      "name" : "Component View: Alexandria RAG System - Scripts Package",
      "order" : 3,
      "relationships" : [ {
        "id" : "12"
      }, {
        "id" : "13"
      }, {
        "id" : "14"
      }, {
        "id" : "15"
      }, {
        "id" : "16"
      }, {
        "id" : "17"
      }, {
        "id" : "18"
      }, {
        "id" : "32"
      }, {
        "id" : "33"
      }, {
        "id" : "34"
      }, {
        "id" : "35"
      }, {
        "id" : "36"
      }, {
        "id" : "37"
      }, {
        "id" : "38"
      } ]
    } ],
    "configuration" : {
      "branding" : { },
      "lastSavedView" : "DetailedIngestionFlow",
      "styles" : {
        "elements" : [ {
          "background" : "#5c2d8a",
          "tag" : "AI"
        }, {
          "background" : "#85bbf0",
          "color" : "#000000",
          "shape" : "Component",
          "tag" : "Component"
        }, {
          "background" : "#438dd5",
          "color" : "#ffffff",
          "shape" : "RoundedBox",
          "tag" : "Container"
        }, {
          "background" : "#999999",
          "color" : "#ffffff",
          "tag" : "External System"
        }, {
          "background" : "#2d8a5f",
          "tag" : "Ingestion"
        }, {
          "background" : "#8a2d58",
          "tag" : "Query"
        }, {
          "background" : "#1168bd",
          "color" : "#ffffff",
          "shape" : "RoundedBox",
          "tag" : "Software System"
        } ]
      },
      "terminology" : { }
    },
    "containerViews" : [ {
      "automaticLayout" : {
        "applied" : false,
        "edgeSeparation" : 0,
        "implementation" : "Graphviz",
        "nodeSeparation" : 300,
        "rankDirection" : "LeftRight",
        "rankSeparation" : 300,
        "vertices" : false
      },
      "elements" : [ {
        "id" : "1",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "3",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "4",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "19",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "20",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "21",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "22",
        "x" : 0,
        "y" : 0
      } ],
      "externalSoftwareSystemBoundariesVisible" : false,
      "key" : "Containers",
      "name" : "Container View: Alexandria RAG System",
      "order" : 2,
      "relationships" : [ {
        "id" : "26"
      }, {
        "id" : "27"
      }, {
        "id" : "28"
      }, {
        "id" : "29"
      }, {
        "id" : "30"
      }, {
        "id" : "31"
      } ],
      "softwareSystemId" : "2"
    } ],
    "dynamicViews" : [ {
      "automaticLayout" : {
        "applied" : false,
        "edgeSeparation" : 0,
        "implementation" : "Graphviz",
        "nodeSeparation" : 300,
        "rankDirection" : "LeftRight",
        "rankSeparation" : 300,
        "vertices" : false
      },
      "description" : "The lifecycle of a book from file to vector",
      "elementId" : "4",
      "elements" : [ {
        "id" : "5",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "6",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "7",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "8",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "10",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "11",
        "x" : 0,
        "y" : 0
      } ],
      "externalBoundariesVisible" : false,
      "key" : "DetailedIngestionFlow",
      "name" : "Dynamic View: Alexandria RAG System - Scripts Package",
      "order" : 4,
      "relationships" : [ {
        "description" : "1. Get file path",
        "id" : "12",
        "order" : "1",
        "response" : false
      }, {
        "description" : "2. Pass raw text",
        "id" : "13",
        "order" : "2",
        "response" : false
      }, {
        "description" : "3. Generate sentence embeddings",
        "id" : "14",
        "order" : "3",
        "response" : false
      }, {
        "description" : "4. Return embeddings for similarity analysis",
        "id" : "14",
        "order" : "4",
        "response" : true
      }, {
        "description" : "5. Generate final chunk embeddings",
        "id" : "14",
        "order" : "5",
        "response" : false
      }, {
        "description" : "6. Prepare embedding batch",
        "id" : "15",
        "order" : "6",
        "response" : false
      }, {
        "description" : "7. Log to Manifest",
        "id" : "16",
        "order" : "7",
        "response" : false
      } ]
    } ],
    "systemContextViews" : [ {
      "automaticLayout" : {
        "applied" : false,
        "edgeSeparation" : 0,
        "implementation" : "Graphviz",
        "nodeSeparation" : 300,
        "rankDirection" : "LeftRight",
        "rankSeparation" : 300,
        "vertices" : false
      },
      "elements" : [ {
        "id" : "1",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "2",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "21",
        "x" : 0,
        "y" : 0
      }, {
        "id" : "22",
        "x" : 0,
        "y" : 0
      } ],
      "enterpriseBoundaryVisible" : true,
      "key" : "SystemContext",
      "name" : "System Context View: Alexandria RAG System",
      "order" : 1,
      "relationships" : [ {
        "id" : "23"
      }, {
        "id" : "24"
      }, {
        "id" : "25"
      } ],
      "softwareSystemId" : "2"
    } ]
  }
}